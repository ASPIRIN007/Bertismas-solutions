{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Overview","text":"<p>Welcome \u2014 this site contains my worked solutions.</p> <ul> <li>Chapter 1</li> <li>Chapter 2</li> <li>Chapter 3</li> </ul>"},{"location":"chapter1/","title":"Chapter 1","text":""},{"location":"chapter1/#notes","title":"Notes","text":"<ul> <li>Notes</li> </ul>"},{"location":"chapter1/#exercises","title":"Exercises","text":"<ul> <li>Exercise 1.1*</li> </ul>"},{"location":"chapter1/ex1-1/","title":"Exercise 1.1*","text":"<p>Suppose \\(f:\\mathbb{R}^n \\to \\mathbb{R}\\) is both concave and convex. Prove \\(f\\) is affine.</p>"},{"location":"chapter1/ex1-1/#solution","title":"Solution","text":"<p>Since \\(f\\) is convex and concave, for any \\(x,y\\) and \\(0\\le \\lambda \\le 1\\), [ f(\\lambda x + (1-\\lambda)y)=\\lambda f(x)+(1-\\lambda)f(y). ]</p> <p>Define \\(g(x)=f(x)-f(0)\\). Then \\(g(0)=0\\) and the equality implies linearity (additivity + homogeneity), so \\(g(x)=a^T x\\) and \\(f(x)=f(0)+a^T x\\), i.e. affine.</p>"},{"location":"chapter1/notes/","title":"Notes","text":"<p>Key box</p> <p>These notes are typed from my handwritten Chapter 1 notes and capture the main definitions + reformulations:</p> <ol> <li>General LP formulation (objective + constraint families)  </li> <li>Turning equalities into inequalities and writing LP in matrix form  </li> <li>Local vs global minima; convexity fact  </li> <li>Piecewise-linear convex objectives and epigraph reformulation  </li> <li>Absolute value modeling (two equivalent LP reformulations)  </li> <li>Graphical (2D) geometry + solution method  </li> <li>Subspaces, span, and affine subspaces  </li> </ol>"},{"location":"chapter1/notes/#introduction-general-lp-form","title":"Introduction: General LP Form","text":"<p>In a general linear programming (LP) problem, we are given a cost vector \\(c=(c_1,\\dots,c_n)\\) and we seek to minimize a linear function \\(c^\\mathsf{T}x=\\sum_{i=1}^n c_i x_i\\) over a decision vector \\(x=(x_1,\\dots,x_n)\\), subject to linear equality/inequality constraints.</p> <p>Let \\(M_1,M_2,M_3\\) be finite index sets. For each constraint \\(i\\), we are given: - an \\(n\\)-dimensional vector \\(a_i\\), - a scalar \\(b_i\\),</p> <p>and these form the \\(i\\)-th linear constraint.</p> <p>Let \\(N_1,N_2 \\subseteq \\{1,\\dots,n\\}\\) indicate which variables are constrained to be nonnegative or nonpositive, respectively.</p> <p>Def box \u2014 General LP</p> <p>\\(\\begin{aligned} \\min\\ &amp; c^\\mathsf{T}x\\\\[2mm] \\text{s.t. }  &amp; a_i^\\mathsf{T}x \\ge b_i, &amp;&amp; i\\in M_1,\\\\ &amp; a_i^\\mathsf{T}x \\le b_i, &amp;&amp; i\\in M_2,\\\\ &amp; a_i^\\mathsf{T}x = b_i, &amp;&amp; i\\in M_3,\\\\ &amp; x_j \\ge 0, &amp;&amp; j\\in N_1,\\\\ &amp; x_j \\le 0, &amp;&amp; j\\in N_2. \\end{aligned}\\)</p> <p>Terminology</p> <ul> <li>Variables \\(x_1,\\dots,x_n\\) are decision variables.  </li> <li>A vector \\(x\\) satisfying all constraints is a feasible solution (feasible vector).  </li> <li>The set of all feasible solutions is the feasible set / feasible region.  </li> <li>If \\(j\\notin N_1\\cup N_2\\), then \\(x_j\\) is a free (unrestricted) variable.  </li> <li>The function \\(c^\\mathsf{T}x\\) is the objective function / cost function.  </li> <li>A feasible solution minimizing the objective is an optimal solution \\(x^\\star\\).  </li> <li>The value \\(c^\\mathsf{T}x^\\star\\) is the optimal cost.</li> </ul>"},{"location":"chapter1/notes/#converting-constraints-matrix-form","title":"Converting Constraints + Matrix Form","text":""},{"location":"chapter1/notes/#equality-as-two-inequalities","title":"Equality as two inequalities","text":"<p>Equality constraint equivalence</p> <p>An equality constraint can be written as two inequalities: \\(a_i^\\mathsf{T}x=b_i \\quad \\Longleftrightarrow \\quad  \\big(a_i^\\mathsf{T}x\\le b_i\\big)\\ \\text{and}\\ \\big(a_i^\\mathsf{T}x\\ge b_i\\big).\\)</p> <p>Also, constraints like \\(x_j\\ge 0\\) or \\(x_j\\le 0\\) can be viewed as special cases of linear inequalities.</p>"},{"location":"chapter1/notes/#expressing-everything-as-one-inequality-direction","title":"Expressing everything as one inequality direction","text":"<p>By multiplying some constraints by \\(-1\\), we can express the feasible set using inequalities of a single direction (e.g. all \u201c\\(\\ge\\)\u201d constraints).</p> <p>Def box \u2014 Matrix form (one-direction inequalities)</p> <p>We can write an LP in matrix form as \\(\\begin{aligned} \\min\\ &amp; c^\\mathsf{T}x\\\\ \\text{s.t. } &amp; Ax \\ge b, \\end{aligned}\\) where \\(A\\) is formed by stacking appropriate row vectors (derived from the \\(a_i^\\mathsf{T}\\)), \\(x=[x_1,\\dots,x_n]^\\mathsf{T}\\), and \\(b=[b_1,\\dots,b_m]^\\mathsf{T}\\).</p>"},{"location":"chapter1/notes/#local-vs-global-minima-convexity-fact","title":"Local vs Global Minima (Convexity Fact)","text":"<p>Def box \u2014 Local minimum</p> <p>A vector \\(x\\) is a local minimum of \\(f\\) if \\(f(x)\\le f(y)\\) for all \\(y\\) in a neighborhood of \\(x\\).</p> <p>Def box \u2014 Global minimum</p> <p>A vector \\(x\\) is a global minimum of \\(f\\) if \\(f(x)\\le f(y)\\) for all \\(y\\).</p> <p>Key fact (convexity)</p> <p>A convex function cannot have a local minimum that is not a global minimum.</p>"},{"location":"chapter1/notes/#piecewise-linear-convex-functions","title":"Piecewise-Linear Convex Functions","text":"<p>Let \\(c_1,\\dots,c_m\\in\\mathbb{R}^n\\) be vectors and \\(d_1,\\dots,d_m\\in\\mathbb{R}\\) be scalars. Consider \\(f(x)=\\max_{i=1,\\dots,m}\\big(c_i^\\mathsf{T}x+d_i\\big).\\)</p> <p>Key box</p> <p>The function \\(f(x)=\\max_i (c_i^\\mathsf{T}x+d_i)\\) is convex and is called a piecewise-linear convex function.</p> <p>Piecewise-linear convex functions can be used to approximate more general convex functions.</p>"},{"location":"chapter1/notes/#lp-with-piecewise-linear-convex-objective-epigraph-trick","title":"LP with piecewise-linear convex objective (epigraph trick)","text":"<p>Consider the problem: \\(\\min\\ \\max_{i=1,\\dots,m}(c_i^\\mathsf{T}x+d_i) \\quad \\text{s.t.}\\quad Ax\\ge b.\\)</p> <p>Introduce \\(z\\) as the smallest number satisfying \\(z\\ge c_i^\\mathsf{T}x+d_i\\) for all \\(i\\). Then we can reformulate as an LP:</p> <p>Epigraph reformulation</p> <p>\\(\\begin{aligned} \\min\\ &amp; z\\\\ \\text{s.t. } &amp; z \\ge c_i^\\mathsf{T}x+d_i,\\quad i=1,\\dots,m,\\\\ &amp; Ax\\ge b. \\end{aligned}\\)</p>"},{"location":"chapter1/notes/#constraint-of-the-form-fxle-h","title":"Constraint of the form \\(f(x)\\le h\\)","text":"<p>If \\(f\\) is piecewise-linear convex, \\(f(x)=\\max_{i=1,\\dots,m}(f_i^\\mathsf{T}x+g_i),\\) then a constraint \\(f(x)\\le h\\) can be written as the system: \\(f_i^\\mathsf{T}x+g_i\\le h,\\quad i=1,\\dots,m.\\)</p>"},{"location":"chapter1/notes/#problems-involving-absolute-values","title":"Problems Involving Absolute Values","text":"<p>Consider: \\(\\min \\sum_{i=1}^n c_i|x_i| \\quad \\text{s.t.}\\quad Ax\\ge b.\\)</p>"},{"location":"chapter1/notes/#lp-formulation-a-introduce-z_i-absolute-value-linearization","title":"LP formulation A: introduce \\(z_i\\) (absolute value linearization)","text":"<p>Absolute value via \\(z_i\\)</p> <p>Introduce \\(z_i\\ge 0\\) such that \\(z_i\\ge |x_i|\\) using: \\(x_i \\le z_i,\\qquad -x_i \\le z_i,\\qquad i=1,\\dots,n.\\) Then the LP becomes: \\(\\begin{aligned} \\min\\ &amp; \\sum_{i=1}^n c_i z_i\\\\ \\text{s.t. } &amp; Ax\\ge b,\\\\ &amp; x_i \\le z_i,\\ \\ -x_i \\le z_i,\\quad i=1,\\dots,n. \\end{aligned}\\)</p>"},{"location":"chapter1/notes/#lp-formulation-b-split-variables-xx-x-","title":"LP formulation B: split variables \\(x=x^+-x^-\\)","text":"<p>Write each \\(x_i\\) as difference of two nonnegative variables: \\(x_i=x_i^+-x_i^-\\) with \\(x_i^+,x_i^-\\ge 0\\). Then \\(|x_i|=x_i^++x_i^-\\).</p> <p>Absolute value via split variables</p> <p>Replace: \\(x = x^+ - x^-,\\) and \\(|x_i| = x_i^+ + x_i^-.\\) Then: \\(\\begin{aligned} \\min\\ &amp; \\sum_{i=1}^n c_i(x_i^+ + x_i^-)\\\\ \\text{s.t. } &amp; A x^+ - A x^- \\ge b,\\\\ &amp; x^+\\ge 0,\\ \\ x^-\\ge 0. \\end{aligned}\\)</p>"},{"location":"chapter1/notes/#graphical-representation-and-solution-2d","title":"Graphical Representation and Solution (2D)","text":""},{"location":"chapter1/notes/#core-geometry","title":"Core geometry","text":"<p>Let the feasible set be \\(P=\\{x\\mid Ax\\le b\\}.\\)</p> <p>Key box \u2014 Core geometry</p> <ul> <li>\\(P\\) is a polyhedron (intersection of half-spaces).  </li> <li>In 2D, \\(P\\) is a polygonal region (possibly unbounded).  </li> <li>Each inequality \\(a_i^\\mathsf{T}x\\le b_i\\) defines a half-plane.  </li> <li>The boundary \\(a_i^\\mathsf{T}x=b_i\\) is a line.</li> </ul> <p>Def box \u2014 Corner/vertex</p> <p>A corner / vertex is a point in \\(P\\) that cannot be written as a nontrivial convex combination of other feasible points.</p> <p>Def box \u2014 Edge/face</p> <p>An edge/face is the set of feasible points where some constraints hold with equality.</p>"},{"location":"chapter1/notes/#objective-geometry-and-the-graphical-method-2d","title":"Objective geometry and the graphical method (2D)","text":"<p>Consider objective level sets: \\(c^\\mathsf{T}x=\\alpha.\\)</p> <p>Graphical method (2D)</p> <ol> <li>Plot each constraint boundary line and choose the correct feasible half-plane.  </li> <li>Find the intersection polygon (the feasible region).  </li> <li>Slide the objective line \\(c^\\mathsf{T}x=\\alpha\\) outward (in the improving direction) until the last point of contact with the feasible region.  </li> <li>That last contact point (or contact edge) gives the optimum.</li> </ol>"},{"location":"chapter1/notes/#subspaces-and-span","title":"Subspaces and Span","text":"<p>Def box \u2014 Subspace</p> <p>A nonempty subset \\(S\\subseteq \\mathbb{R}^n\\) is a subspace of \\(\\mathbb{R}^n\\) if for every \\(x,y\\in S\\) and every \\(a,b\\in\\mathbb{R}\\), \\(ax+by\\in S.\\) If \\(S\\ne \\mathbb{R}^n\\), then \\(S\\) is a proper subspace.</p> <p>Key property</p> <p>Every subspace contains the zero vector.</p> <p>Def box \u2014 Span</p> <p>The span of vectors \\(y^1,\\dots,y^K\\in\\mathbb{R}^n\\) is the set \\(\\mathrm{span}\\{y^1,\\dots,y^K\\}   =\\left\\{\\sum_{k=1}^K a_k y^k \\;:\\; a_k\\in\\mathbb{R}\\right\\}.\\)</p>"},{"location":"chapter1/notes/#affine-subspaces","title":"Affine Subspaces","text":"<p>Let \\(S_0\\) be a subspace of \\(\\mathbb{R}^n\\) and let \\(x^0\\in\\mathbb{R}^n\\). Define: \\(S = S_0 + x^0 = \\{s + x^0 \\mid s\\in S_0\\}.\\)</p> <p>Def box \u2014 Affine subspace</p> <p>In general, \\(S\\) is not a subspace because it may not contain the zero vector. Such a translated set \\(S=S_0+x^0\\) is called an affine subspace.</p>"},{"location":"chapter2/","title":"Chapter 2","text":""},{"location":"chapter2/#notes","title":"Notes","text":"<ul> <li>Notes</li> </ul>"},{"location":"chapter2/#doubts","title":"Doubts","text":"<ul> <li>Doubts</li> </ul>"},{"location":"chapter2/doubts/","title":"Doubt for theorem 2.5","text":""},{"location":"chapter2/doubts/#example-a-good-case-redundant-equalities-can-be-removed-when-the-feasible-set-is-nonempty","title":"Example A (Good case): redundant equalities can be removed when the feasible set is nonempty","text":"<p>We build a \\(4\\times 7\\) matrix \\(A\\) with dependent rows (rank \\(2&lt;4\\)). Rows 3 and 4 are linear combinations of rows 1 and 2.</p> <p>Let - \\(r_1 = [1,\\ 0,\\ 2,\\ 0,\\ 0,\\ 0,\\ 0]\\) - \\(r_2 = [0,\\ 1,\\ 1,\\ 0,\\ 0,\\ 0,\\ 0]\\) - \\(r_3 = r_1+r_2 = [1,\\ 1,\\ 3,\\ 0,\\ 0,\\ 0,\\ 0]\\) - \\(r_4 = 2r_1-r_2 = [2,\\ -1,\\ 3,\\ 0,\\ 0,\\ 0,\\ 0]\\)</p> <p>So \\(A= \\begin{bmatrix} 1&amp;0&amp;2&amp;0&amp;0&amp;0&amp;0\\\\ 0&amp;1&amp;1&amp;0&amp;0&amp;0&amp;0\\\\ 1&amp;1&amp;3&amp;0&amp;0&amp;0&amp;0\\\\ 2&amp;-1&amp;3&amp;0&amp;0&amp;0&amp;0 \\end{bmatrix}.\\)</p> <p>Now choose a feasible point \\(x\\ge 0\\), for example \\(x= \\begin{bmatrix} 2\\\\ 1\\\\ 3\\\\ 0\\\\ 0\\\\ 0\\\\ 0 \\end{bmatrix}.\\)</p> <p>Compute \\(b=Ax\\): - Row 1: \\(1(2)+2(3)=8\\) - Row 2: \\(1(1)+1(3)=4\\) - Row 3: \\(8+4=12\\) - Row 4: \\(2\\cdot 8-4=12\\)</p> <p>Hence \\(b= \\begin{bmatrix} 8\\\\ 4\\\\ 12\\\\ 12 \\end{bmatrix}.\\)</p> <p>Define \\(P=\\{x\\in\\mathbb{R}^7 \\mid Ax=b,\\ x\\ge 0\\}.\\)</p> <p>The equality system \\(Ax=b\\) corresponds to: 1. \\(x_1+2x_3=8\\) 2. \\(x_2+x_3=4\\) 3. \\(x_1+x_2+3x_3=12\\) 4. \\(2x_1-x_2+3x_3=12\\) and \\(x\\ge 0\\).</p> <p>Now define the reduced system (keep only rows 1 and 2): \\(Q=\\{x\\in\\mathbb{R}^7 \\mid r_1x=8,\\ r_2x=4,\\ x\\ge 0\\}.\\)</p> <p>Why \\(P=Q\\) (key point): - If \\(x\\in P\\), then \\(x\\) satisfies all four equations, so it satisfies the first two \\(\\Rightarrow P\\subseteq Q\\). - If \\(x\\in Q\\), then:   - Adding eqn (1)+(2) gives \\((r_1+r_2)x=8+4=12\\), but \\(r_1+r_2=r_3\\), so eqn (3) holds automatically.   - Taking \\(2\\cdot\\)(1)\\(-\\)(2) gives \\((2r_1-r_2)x=16-4=12\\), but \\(2r_1-r_2=r_4\\), so eqn (4) holds automatically.   Hence \\(x\\) satisfies all four equations \\(\\Rightarrow Q\\subseteq P\\).</p> <p>Therefore \\(P=Q\\).</p>"},{"location":"chapter2/doubts/#example-b-bad-case-if-the-feasible-set-is-empty-dropping-redundant-rows-can-change-the-set","title":"Example B (Bad case): if the feasible set is empty, dropping \u201credundant\u201d rows can change the set","text":"<p>Use the same \\(A\\) as above (rows 3 and 4 are linear combinations of rows 1 and 2): \\(A= \\begin{bmatrix} 1&amp;0&amp;2&amp;0&amp;0&amp;0&amp;0\\\\ 0&amp;1&amp;1&amp;0&amp;0&amp;0&amp;0\\\\ 1&amp;1&amp;3&amp;0&amp;0&amp;0&amp;0\\\\ 2&amp;-1&amp;3&amp;0&amp;0&amp;0&amp;0 \\end{bmatrix}.\\)</p> <p>Now choose an inconsistent right-hand side \\(b\\) by violating \\(b_3=b_1+b_2\\): \\(b= \\begin{bmatrix} 8\\\\ 4\\\\ 13\\\\ 12 \\end{bmatrix}.\\)</p> <p>Define \\(P=\\{x\\in\\mathbb{R}^7 \\mid Ax=b,\\ x\\ge 0\\}.\\)</p> <p>Why \\(P=\\emptyset\\): - From rows 1 and 2, any \\(x\\) satisfying them must satisfy \\((r_1+r_2)x=8+4=12\\). - But \\(r_1+r_2=r_3\\), so this forces \\(r_3x=12\\). - Row 3 also requires \\(r_3x=13\\). - Contradiction (\\(12\\ne 13\\)), so no \\(x\\) can satisfy \\(Ax=b\\).</p> <p>Now drop rows 3 and 4 and define \\(Q=\\{x\\in\\mathbb{R}^7 \\mid r_1x=8,\\ r_2x=4,\\ x\\ge 0\\}.\\)</p> <p>\\(Q\\) is nonempty: Take \\(x_3=0\\), then eqn (1) gives \\(x_1=8\\) and eqn (2) gives \\(x_2=4\\). Set the remaining variables to zero: \\(x= \\begin{bmatrix} 8\\\\ 4\\\\ 0\\\\ 0\\\\ 0\\\\ 0\\\\ 0 \\end{bmatrix}\\ge 0.\\)</p> <p>So \\(Q\\ne\\emptyset\\) but \\(P=\\emptyset\\).</p> <p>Conclusion: If the original feasible set is empty, removing \u201cdependent\u201d equality constraints can create new feasible points, so the nonempty assumption in Theorem 2.5 is necessary.</p>"},{"location":"chapter2/doubts/#degeneracy-can-depend-on-how-we-represent-the-same-feasible-set","title":"Degeneracy can depend on how we represent the same feasible set","text":"<p>Key box</p> <p>These examples show an important subtlety:</p> <ul> <li>The same geometric polyhedron can sometimes be written with different constraint sets.  </li> <li>Under one representation, a BFS may look degenerate (too many active constraints).  </li> <li>Under another representation, the same point may look nondegenerate.</li> </ul>"},{"location":"chapter2/doubts/#example-1-same-feasible-set-different-standard-form-representations","title":"Example 1: Same feasible set, different \u201cstandard form\u201d representations","text":"<p>Consider the (standard form style) polyhedron \\(P=\\{(x_1,x_2,x_3)\\mid x_1-x_2=0,\\ x_1+x_2+2x_3=2,\\ x_1,x_2,x_3\\ge 0\\}.\\)</p> <p>Here \\(n=3\\), there are \\(m=2\\) equality constraints, so \\(n-m=1\\). In standard form, a nondegenerate BFS should have exactly \\(n-m=1\\) variable equal to zero.</p> <ul> <li>The point \\((1,1,0)\\) is nondegenerate because exactly one variable is zero (\\(x_3=0\\)).</li> <li>The point \\((0,0,1)\\) is degenerate because two variables are zero (\\(x_1=x_2=0\\)).</li> </ul> <p>Now write the same polyhedron using fewer nonnegativity constraints: \\(P=\\{(x_1,x_2,x_3)\\mid x_1-x_2=0,\\ x_1+x_2+2x_3=2,\\ x_1\\ge 0,\\ x_3\\ge 0\\}.\\)</p> <p>What changed?</p> <p>We dropped the explicit constraint \\(x_2\\ge 0\\). (Because \\(x_2=x_1\\) from \\(x_1-x_2=0\\), the nonnegativity of \\(x_2\\) is implied by \\(x_1\\ge 0\\).)</p> <p>Under this second representation, the point \\((0,0,1)\\) becomes nondegenerate: - At \\((0,0,1)\\), we now have only three active constraints:   - \\(x_1-x_2=0\\) (active as equality)   - \\(x_1+x_2+2x_3=2\\) (active as equality)   - \\(x_1\\ge 0\\) active because \\(x_1=0\\) - There is no longer an explicit \\(x_2\\ge 0\\) constraint to be active.</p> <p>So the \u201cdegeneracy label\u201d of the same BFS can change when we rewrite constraints.</p>"},{"location":"chapter2/doubts/#example-2-adding-redundant-inequalities-can-force-degeneracy","title":"Example 2: Adding redundant inequalities can force degeneracy","text":"<p>Start with a standard form polyhedron \\(P=\\{x\\mid Ax=b,\\ x\\ge 0\\}\\) with \\(A\\in\\mathbb{R}^{m\\times n}\\).</p> <p>Let \\(x^*\\) be a nondegenerate BFS under this representation. That means: - exactly \\(n-m\\) components of \\(x^*\\) are equal to \\(0\\), and - the number of active constraints at \\(x^*\\) is exactly \\(n\\).</p> <p>Now represent the same feasible set by converting \\(Ax=b\\) into two-sided inequalities: \\(P=\\{x\\mid Ax\\ge b,\\ -Ax\\ge -b,\\ x\\ge 0\\}.\\)</p> <p>What happens at \\(x^*\\) now?</p> <p>At any feasible point, \\(Ax=b\\) implies both \\(Ax\\ge b\\) and \\(-Ax\\ge -b\\) hold with equality. So we add \\(2m\\) active inequality constraints automatically.</p> <p>At the point \\(x^*\\): - we still have the \\(n-m\\) active constraints from \\(x\\ge 0\\) (the zero variables), - and now we also have \\(2m\\) active constraints from the two-sided inequality representation of \\(Ax=b\\).</p> <p>Total active constraints at \\(x^*\\): \\((n-m) + 2m = n+m,\\) which is strictly larger than \\(n\\) (since \\(m&gt;0\\)).</p> <p>Therefore, under this second representation, \\(x^*\\) becomes degenerate.</p> <p>Conclusion</p> <p>A BFS that is degenerate under one representation might be nondegenerate under another.</p> <p>However (important caveat): if a BFS is degenerate under one standard form representation, it can be shown it remains degenerate under every standard form representation of the same polyhedron.</p>"},{"location":"chapter2/doubts/#confusion-between-general-and-standard-form-of-polyhedron-and-number-of-constraints","title":"Confusion between general and standard form of polyhedron and number of constraints","text":"<p>Clarification: \\(m&lt;n\\) does not mean \u201cno feasible points\u201d</p> <p>In the paragraph after Definition 2.9, \\(m\\) denotes the total number of constraints used to define a polyhedron in \\(\\mathbb{R}^n\\) (equalities + inequalities).</p> <p>A basic solution (Definition 2.9) requires that among the constraints active at \\(x^\\*\\) there are \\(n\\) linearly independent ones. Therefore, at any point, \\(\\#\\{\\text{active constraints}\\}\\le m.\\)</p> <p>If \\(m&lt;n\\), then \\(\\#\\{\\text{active constraints at }x\\}&lt;n\\) for every \\(x\\), so it is impossible to have \\(n\\) linearly independent active constraints. Hence there are no basic solutions and no basic feasible solutions.</p> <p>This does not mean that the feasible set is empty. The polyhedron may still have feasible points (often infinitely many); it simply has no \u201ccorners/vertices\u201d in the sense of Definition 2.9.</p> <p>Why this is not contradictory with standard form (\\(Ax=b,\\ x\\ge 0\\))</p> <p>In standard form, \\(P=\\{x\\in\\mathbb{R}^n \\mid Ax=b,\\ x\\ge 0\\},\\) the total number of constraints is \\(m+n\\) (the \\(m\\) equalities plus the \\(n\\) nonnegativity inequalities). Thus there are always at least \\(n\\) constraints available, so basic/basic-feasible solutions can exist, and the construction \u201cset \\(n-m\\) variables to zero\u201d makes sense.</p>"},{"location":"chapter2/notes2/","title":"Notes","text":""},{"location":"chapter2/notes2/#21-polyhedra-and-convex-sets","title":"2.1 Polyhedra and Convex Sets","text":"<p>Key box</p> <p>Section 2.1 sets up the geometry language used throughout LP:</p> <ol> <li>A polyhedron is the feasible set of finitely many linear inequalities  </li> <li>Bounded vs unbounded sets (can the feasible region extend to infinity?)  </li> <li>Hyperplanes (equalities) and halfspaces (inequalities)  </li> <li>Convexity: the line segment between feasible points stays feasible  </li> <li>Convex combinations and the convex hull </li> <li>Closure facts (Theorem 2.1): intersections preserve convexity; polyhedra are convex  </li> </ol>"},{"location":"chapter2/notes2/#polyhedra-feasible-sets-of-lp-constraints","title":"Polyhedra (feasible sets of LP constraints)","text":"<p>Def box \u2014 Polyhedron</p> <p>A polyhedron is any set that can be written as \\(P=\\{x\\in\\mathbb{R}^n \\mid Ax \\ge b\\},\\) i.e., the set of vectors satisfying finitely many linear inequalities.</p> <p>Interpretation: each inequality is a \u201ccut\u201d of the space; the feasible region is what remains after applying all cuts.</p>"},{"location":"chapter2/notes2/#boundedness","title":"Boundedness","text":"<p>Def box \u2014 Bounded set</p> <p>A set \\(S\\subseteq\\mathbb{R}^n\\) is bounded if there exists a constant \\(K\\) such that \\(|x_i|\\le K\\) for every \\(x\\in S\\) and each component \\(i\\).</p> <ul> <li>Bounded polyhedron: trapped inside some big box.  </li> <li>Unbounded polyhedron: you can move infinitely far in some direction while staying feasible.</li> </ul> <p>Why it matters later: unboundedness can lead to LPs with no finite optimum (depending on the objective direction).</p>"},{"location":"chapter2/notes2/#hyperplanes-and-halfspaces-single-linear-constraints","title":"Hyperplanes and Halfspaces (single linear constraints)","text":"<p>Let \\(a\\neq 0\\) and \\(b\\in\\mathbb{R}\\).</p> <p>Def box \u2014 Hyperplane</p> <p>The hyperplane with normal \\(a\\) and offset \\(b\\) is \\(H=\\{x\\in\\mathbb{R}^n \\mid a^\\mathsf{T}x=b\\}.\\)</p> <p>Def box \u2014 Halfspace</p> <p>The halfspace defined by the inequality is \\(S=\\{x\\in\\mathbb{R}^n \\mid a^\\mathsf{T}x\\ge b\\}.\\)</p> <p>Key geometric facts - The hyperplane \\(a^\\mathsf{T}x=b\\) is the boundary of the halfspace \\(a^\\mathsf{T}x\\ge b\\). - The vector \\(a\\) is perpendicular (normal) to the hyperplane:</p> <p>If \\(x,y\\in H\\), then \\(a^\\mathsf{T}x=b\\) and \\(a^\\mathsf{T}y=b\\), so   \\(a^\\mathsf{T}(x-y)=0,\\)   meaning any direction along the hyperplane is orthogonal to \\(a\\).</p> <p>Polyhedron viewpoint \\(P=\\{x\\mid Ax\\ge b\\}\\) is the intersection of finitely many halfspaces (one per row of \\(A\\)).</p>"},{"location":"chapter2/notes2/#convex-sets-the-main-structural-property","title":"Convex Sets (the main structural property)","text":"<p>Def box \u2014 Convex set</p> <p>A set \\(S\\subseteq\\mathbb{R}^n\\) is convex if for any \\(x,y\\in S\\) and any \\(\\lambda\\in[0,1]\\), \\(\\lambda x + (1-\\lambda)y\\in S.\\) (Equivalently: the whole line segment between any two points in \\(S\\) lies in \\(S\\).)</p> <p>Why convexity matters for LP - If \\(x\\) and \\(y\\) are feasible, then any \u201cmix\u201d of them is feasible. - This property is fundamental for later results like \u201can optimum occurs at a corner/extreme point.\u201d</p>"},{"location":"chapter2/notes2/#convex-combinations-and-convex-hull","title":"Convex combinations and convex hull","text":"<p>Def box \u2014 Convex combination</p> <p>A vector \\(z\\) is a convex combination of \\(x^1,\\dots,x^k\\) if \\(z=\\sum_{i=1}^k \\lambda_i x^i,\\) where \\(\\lambda_i\\ge 0\\) and \\(\\sum_{i=1}^k \\lambda_i = 1.\\)</p> <p>Def box \u2014 Convex hull</p> <p>The convex hull of points \\(x^1,\\dots,x^k\\) is the set of all their convex combinations: \\(\\mathrm{conv}\\{x^1,\\dots,x^k\\} = \\left\\{\\sum_{i=1}^k \\lambda_i x^i \\; \\middle|\\; \\lambda_i\\ge 0,\\ \\sum_{i=1}^k\\lambda_i=1\\right\\}.\\) It is the smallest convex set containing those points.</p>"},{"location":"chapter2/notes2/#theorem-21-closure-properties-youll-use-repeatedly","title":"Theorem 2.1 (closure properties you\u2019ll use repeatedly)","text":"<p>Theorem box \u2014 Basic convexity facts</p> <ol> <li>The intersection of convex sets is convex.  </li> <li>Every polyhedron is convex.    (Halfspaces are convex, and a polyhedron is an intersection of halfspaces.)  </li> <li>If \\(S\\) is convex and \\(x^1,\\dots,x^k\\in S\\), then every convex combination \\(\\sum_{i=1}^k \\lambda_i x^i\\) lies in \\(S\\).  </li> <li>The convex hull of finitely many points is convex.</li> </ol> <p>Proof idea (what to remember)</p> <ul> <li>Halfspace convexity: if \\(a^\\mathsf{T}x\\ge b\\) and \\(a^\\mathsf{T}y\\ge b\\), then \\(a^\\mathsf{T}(\\lambda x+(1-\\lambda)y)   =\\lambda a^\\mathsf{T}x+(1-\\lambda)a^\\mathsf{T}y   \\ge \\lambda b+(1-\\lambda)b=b.\\) </li> <li>Intersections preserve convexity: if the segment stays in each set, it stays in their intersection.</li> </ul> <p>Section 2.1 \u2014 Exam checklist</p> <ul> <li>A feasible set of linear inequalities is a polyhedron: \\(P=\\{x\\mid Ax\\ge b\\}\\).  </li> <li>Each inequality defines a halfspace; equalities define hyperplanes.  </li> <li>Polyhedra are convex (intersection of convex halfspaces).  </li> <li>Bounded means contained in a box; unbounded means it extends to infinity.  </li> <li>Convex combination and convex hull formalize \u201cmixing\u201d feasible points.</li> </ul>"},{"location":"chapter2/notes2/#22-extreme-points-vertices-and-basic-feasible-solutions","title":"2.2 Extreme Points, Vertices, and Basic Feasible Solutions","text":"<p>Key box</p> <p>Section 2.2 formalizes what a \u201ccorner\u201d of a polyhedron means, in three equivalent ways:</p> <ol> <li>Extreme point (purely geometric: cannot be written as a nontrivial convex combination)  </li> <li>Vertex (optimization view: unique optimizer for some linear objective)  </li> <li>Basic feasible solution (BFS) (algebraic test: \\(n\\) linearly independent active constraints)  </li> </ol> <p>Main result: Extreme points = vertices = BFS (Theorem 2.2). It also defines adjacent basic solutions (sets up simplex \u201cmove along an edge\u201d).</p>"},{"location":"chapter2/notes2/#221-extreme-points","title":"2.2.1 Extreme points","text":"<p>Def box \u2014 Extreme point</p> <p>Let \\(P\\) be a convex set (in particular, a polyhedron). A point \\(x\\in P\\) is an extreme point of \\(P\\) if it cannot be expressed as a nontrivial convex combination of two distinct points of \\(P\\).</p> <p>Equivalently: if \\(y,z\\in P\\) and \\(0\\le \\lambda \\le 1\\) satisfy \\(x=\\lambda y+(1-\\lambda)z,\\) then either \\(x=y\\), or \\(x=z\\), or \\(\\lambda\\in\\{0,1\\}\\).</p> <p>Geometric meaning: extreme points are the true \u201ccorners\u201d \u2014 you cannot obtain them by \u201cmixing\u201d two other feasible points.</p>"},{"location":"chapter2/notes2/#222-vertices-supporting-hyperplane-lp-viewpoint","title":"2.2.2 Vertices (supporting-hyperplane / LP viewpoint)","text":"<p>Def box \u2014 Vertex</p> <p>Let \\(P\\) be a polyhedron. A point \\(x\\in P\\) is a vertex of \\(P\\) if it is the unique optimal solution of some linear program with feasible set \\(P\\).</p> <p>That is, there exists a vector \\(c\\) such that \\(x\\) is the unique minimizer of \\(\\min\\{c^\\mathsf{T}u \\mid u\\in P\\}.\\)</p> <p>Geometric meaning: there is a supporting hyperplane (a level set of a linear objective) that touches \\(P\\) only at \\(x\\).</p>"},{"location":"chapter2/notes2/#223-active-constraints-and-enough-equalities-to-pin-down-a-point","title":"2.2.3 Active constraints and \u201cenough equalities to pin down a point\u201d","text":"<p>Let the polyhedron be written as \\(P=\\{x\\in\\mathbb{R}^n \\mid a_i^\\mathsf{T}x \\ge b_i,\\ i=1,\\dots,m\\}.\\)</p> <p>At a point \\(x\\in P\\), constraint \\(i\\) is active if it holds with equality: \\(a_i^\\mathsf{T}x=b_i.\\)</p> <p>Terminology \u2014 Active set</p> <p>The active set at \\(x\\) is the index set \\(I(x)=\\{i\\in\\{1,\\dots,m\\} \\mid a_i^\\mathsf{T}x=b_i\\}.\\)</p> <p>Key idea: if the active constraints at \\(x\\) include \\(n\\) linearly independent normals, then those equalities determine \\(x\\) uniquely (they \u201cpin down\u201d the point like a corner).</p>"},{"location":"chapter2/notes2/#224-basic-solutions-and-basic-feasible-solutions","title":"2.2.4 Basic solutions and basic feasible solutions","text":"<p>Def box \u2014 Basic feasible solution (BFS)</p> <p>A feasible point \\(x\\in P\\) is a basic feasible solution if among the active constraints at \\(x\\), there exist \\(n\\) linearly independent constraints.</p> <p>(Equivalently: the active constraint normals span \\(\\mathbb{R}^n\\).)</p> <p>Remark \u2014 Basic solution vs BFS</p> <p>Often one forms a basic solution by selecting \\(n\\) linearly independent constraints, solving them as equalities to obtain a unique candidate \\(x\\), and then checking feasibility.</p> <ul> <li>If the resulting \\(x\\) satisfies all constraints, it is a basic feasible solution.  </li> <li>If it violates some constraints, it is a basic solution but not feasible.</li> </ul> <p>Important remark (representation issue)</p> <p>Whether a point is a basic solution can depend on how the polyhedron is written (which constraints you include, redundant constraints, etc.).</p> <p>However, the main theorem below shows that BFS are the same as extreme points, so the property \u201cbeing a BFS\u201d is ultimately geometric (representation-independent).</p>"},{"location":"chapter2/notes2/#225-main-equivalence-theorem","title":"2.2.5 Main equivalence theorem","text":"<p>Theorem box \u2014 Extreme point = vertex = BFS</p> <p>Let \\(P\\) be a polyhedron. For a point \\(x\\in P\\), the following are equivalent:</p> <ol> <li>\\(x\\) is a vertex of \\(P\\).  </li> <li>\\(x\\) is an extreme point of \\(P\\).  </li> <li>\\(x\\) is a basic feasible solution of \\(P\\).  </li> </ol> <p>Why this theorem matters - It justifies the \u201ccorner\u201d viewpoint in three ways:   - geometry: extreme point,   - optimization: vertex,   - algebra: BFS (what algorithms can test and move between).</p>"},{"location":"chapter2/notes2/#226-finiteness-of-basic-solutions","title":"2.2.6 Finiteness of basic solutions","text":"<p>Key fact \u2014 There are finitely many basic solutions</p> <p>A polyhedron defined by finitely many constraints has only finitely many ways to choose \\(n\\) linearly independent constraints.</p> <p>Therefore, the number of basic solutions (and hence BFS / extreme points) is finite.</p>"},{"location":"chapter2/notes2/#227-adjacency-neighbors-and-edges","title":"2.2.7 Adjacency (neighbors) and edges","text":"<p>Def box \u2014 Adjacent basic solutions</p> <p>Two distinct basic solutions are adjacent if they share \\(n-1\\) linearly independent active constraints.</p> <p>Geometric meaning</p> <p>If two basic feasible solutions are adjacent, the line segment joining them lies on an edge of the polyhedron. (This is the geometric backbone of simplex: move from one BFS to an adjacent BFS along an edge.)</p> <p>Section 2.2 \u2014 Exam checklist</p> <ul> <li>Know the three corner notions:</li> <li>Extreme point = cannot be written as a nontrivial convex combination  </li> <li>Vertex = unique optimizer for some linear objective  </li> <li>BFS = \\(n\\) linearly independent active constraints at the point  </li> <li>Be able to state: Extreme point \u21d4 Vertex \u21d4 BFS.  </li> <li>Know adjacency: share \\(n-1\\) independent active constraints (neighbors along an edge).</li> </ul>"},{"location":"chapter2/notes2/#23-polyhedra-in-standard-form","title":"2.3 Polyhedra in Standard Form","text":"<p>Key box</p> <p>Section 2.3 specializes the geometry of Section 2.2 to the standard form feasible set:</p> <ol> <li>Standard form polyhedron: \\(P=\\{x\\mid Ax=b,\\ x\\ge 0\\}\\) </li> <li>Basis and partition into basic vs nonbasic variables  </li> <li>Constructing a basic solution by setting \\(n-m\\) variables to zero  </li> <li>Basic feasible solution (BFS) = basic solution that also satisfies \\(x\\ge 0\\) </li> <li>A basic solution has at most \\(m\\) nonzero components  </li> <li>Adjacent bases (differ by one column) correspond to \u201cneighbor\u201d corners (sets up simplex)  </li> <li>Assuming \\(\\mathrm{rank}(A)=m\\) is no loss of generality (redundant equalities can be removed)</li> </ol>"},{"location":"chapter2/notes2/#standard-form-feasible-set","title":"Standard form feasible set","text":"<p>Def box \u2014 Standard form polyhedron</p> <p>A standard form feasible set is \\(P=\\{x\\in\\mathbb{R}^n \\mid Ax=b,\\ x\\ge 0\\},\\) where \\(A\\in\\mathbb{R}^{m\\times n}\\) and \\(b\\in\\mathbb{R}^m\\).</p> <p>Standing assumption (used throughout)</p> <p>Often we assume \\(\\mathrm{rank}(A)=m\\) (the \\(m\\) equality constraints are linearly independent) and \\(m\\le n\\).</p>"},{"location":"chapter2/notes2/#bases-and-basic-variables","title":"Bases and basic variables","text":"<p>Let \\(A_1,\\dots,A_n\\) denote the columns of \\(A\\).</p> <p>Def box \u2014 Basis (column basis)</p> <p>A set of indices \\(B\\subseteq\\{1,\\dots,n\\}\\) with \\(|B|=m\\) is a basis if the columns \\(\\{A_j\\}_{j\\in B}\\) are linearly independent.</p> <p>Notation</p> <ul> <li>The basis matrix is \\(A_B\\in\\mathbb{R}^{m\\times m}\\) formed by the columns indexed by \\(B\\).  </li> <li>The remaining indices are \\(N=\\{1,\\dots,n\\}\\setminus B\\).  </li> <li>Variables \\(\\{x_j\\}_{j\\in B}\\) are basic variables; variables \\(\\{x_j\\}_{j\\in N}\\) are nonbasic variables.</li> </ul>"},{"location":"chapter2/notes2/#basic-solutions","title":"Basic solutions","text":"<p>In standard form, the equalities \\(Ax=b\\) are always active, and the only inequalities are \\(x\\ge 0\\). A \u201ccorner candidate\u201d is obtained by activating \\(n-m\\) of the nonnegativity constraints, i.e., setting \\(n-m\\) variables to zero.</p> <p>Def box \u2014 Basic solution (associated with a basis \\(B\\))</p> <p>Given a basis \\(B\\) (so \\(A_B\\) is invertible), define a vector \\(x\\) by \\(x_N=0,\\) and \\(A_B x_B=b,\\) i.e., \\(x_B=A_B^{-1}b.\\) This \\(x\\) is called the basic solution associated with \\(B\\).</p> <p>Key property</p> <p>Any basic solution has at most \\(m\\) nonzero components, because \\(x_N=0\\) and only the \\(m\\) basic variables can be nonzero.</p>"},{"location":"chapter2/notes2/#basic-feasible-solutions-bfs","title":"Basic feasible solutions (BFS)","text":"<p>Def box \u2014 Basic feasible solution (BFS)</p> <p>A basic feasible solution is a basic solution that is feasible: \\(x\\ge 0.\\)</p> <p>Interpretation</p> <ul> <li>A basis always produces a basic solution via \\(x_B=A_B^{-1}b,\\ x_N=0\\).  </li> <li>That basic solution may be infeasible (some component negative).  </li> <li>If it satisfies \\(x\\ge 0\\), it is a BFS (a \u201ccorner\u201d of \\(P\\)).</li> </ul>"},{"location":"chapter2/notes2/#bases-vs-basic-solutions-not-always-one-to-one","title":"Bases vs basic solutions (not always one-to-one)","text":"<p>Remark</p> <p>Different bases can sometimes lead to the same basic solution. This typically happens when the resulting BFS has some basic variables equal to zero (degeneracy). (Example: if \\(b=0\\), then every basis yields the basic solution \\(x=0\\).)</p>"},{"location":"chapter2/notes2/#adjacency-in-standard-form-sets-up-simplex-moves","title":"Adjacency in standard form (sets up simplex moves)","text":"<p>Def box \u2014 Adjacent bases</p> <p>Two bases \\(B\\) and \\(B'\\) are adjacent if they differ in exactly one index, i.e., \\(|B\\cap B'|=m-1.\\)</p> <p>Geometric meaning</p> <p>Adjacent bases correspond to swapping one basic variable with one nonbasic variable. When this changes the basic solution, it moves to a neighboring corner along an edge of the feasible set.</p>"},{"location":"chapter2/notes2/#full-row-rank-assumption-is-no-loss-of-generality","title":"Full row rank assumption is no loss of generality","text":"<p>Theorem box \u2014 Removing redundant equalities</p> <p>Consider \\(P=\\{x\\mid Ax=b,\\ x\\ge 0\\}\\) and suppose \\(P\\) is nonempty. If \\(\\mathrm{rank}(A)=k&lt;m\\), then some equality constraints are redundant. There exists a matrix \\(D\\in\\mathbb{R}^{k\\times n}\\) with \\(\\mathrm{rank}(D)=k\\) and a vector \\(f\\in\\mathbb{R}^k\\) such that \\(\\{x\\mid Ax=b,\\ x\\ge 0\\}=\\{x\\mid Dx=f,\\ x\\ge 0\\}.\\)</p> <p>Takeaway</p> <p>We can assume \\(\\mathrm{rank}(A)=m\\) (independent equality constraints) without changing the feasible set, as long as the feasible set is nonempty.</p> <p>Section 2.3 \u2014 Exam checklist</p> <ul> <li>Standard form: \\(P=\\{x\\mid Ax=b,\\ x\\ge 0\\}\\).  </li> <li>Basis \\(B\\): choose \\(m\\) linearly independent columns \\(\\Rightarrow A_B\\) invertible.  </li> <li>Basic solution: \\(x_N=0\\), \\(x_B=A_B^{-1}b\\).  </li> <li>BFS: basic solution with \\(x\\ge 0\\).  </li> <li>Any basic solution has at most \\(m\\) nonzeros.  </li> <li>Adjacent bases differ by one column (simplex \u201cpivot\u201d idea).  </li> <li>Redundant equalities can be removed \\(\\Rightarrow\\) assume \\(\\mathrm{rank}(A)=m\\).</li> </ul>"},{"location":"chapter2/notes2/#24-degeneracy","title":"2.4 Degeneracy","text":"<p>Key box</p> <p>In Section 2.4 we study degeneracy, i.e., when a \u201ccorner\u201d is pinned down by more constraints than necessary.</p> <ol> <li>A basic solution in \\(\\mathbb{R}^n\\) is determined by (at least) \\(n\\) active constraints  </li> <li>Degenerate means more than \\(n\\) constraints are active at the basic solution  </li> <li>In standard form \\(Ax=b,\\ x\\ge 0\\): degeneracy shows up as extra zero variables beyond the required \\(n-m\\) </li> <li>Consequence: multiple bases can represent the same BFS (important later for simplex behavior)</li> </ol>"},{"location":"chapter2/notes2/#active-constraints-at-a-point","title":"Active constraints at a point","text":"<p>For a polyhedron \\(P=\\{x\\in\\mathbb{R}^n \\mid a_i^\\mathsf{T}x\\ge b_i,\\ i=1,\\dots,m\\}\\), constraint \\(i\\) is active at \\(x\\) if \\(a_i^\\mathsf{T}x=b_i\\).</p> <p>Def box \u2014 Degenerate / nondegenerate basic feasible solution</p> <p>Let \\(P\\subseteq \\mathbb{R}^n\\) be a polyhedron and let \\(x\\) be a basic feasible solution of \\(P\\).</p> <ul> <li>\\(x\\) is nondegenerate if it has exactly \\(n\\) active constraints.  </li> <li>\\(x\\) is degenerate if it has more than \\(n\\) active constraints.</li> </ul> <p>Standard form interpretation</p> <p>For \\(P=\\{x\\mid Ax=b,\\ x\\ge 0\\}\\) with \\(A\\in\\mathbb{R}^{m\\times n}\\) and \\(\\mathrm{rank}(A)=m\\): - The \\(m\\) equalities \\(Ax=b\\) are always active. - You need \\(n-m\\) more active constraints from \\(x_i\\ge 0\\), i.e., at least \\(n-m\\) variables equal to \\(0\\). - A BFS is degenerate if more than \\(n-m\\) variables are zero (i.e., \u201cextra\u201d nonnegativity constraints are active).</p> <p>Example (typical degeneracy pattern)</p> <p>In standard form, if \\(n-m=3\\) but at a BFS you observe \\(4\\) (or more) zero components, then the BFS is degenerate.</p> <p>Why degeneracy matters</p> <p>Degeneracy is the main reason: - distinct bases can yield the same BFS, and - Degeneracy of a basic feasible solution is not in general, a geometric property but rather it may depend on the particular representation of polyhedron.  - A basic feasible solution which is non-degenerate under one representation can be degenerate under other representatio.(See doubts section)</p>"},{"location":"chapter2/notes2/#25-existence-of-extreme-points","title":"2.5 Existence of Extreme Points","text":"<p>Key box</p> <p>This section answers: When does a nonempty polyhedron have at least one extreme point?</p> <ol> <li>Introduce the idea of a polyhedron containing a line </li> <li>Main criterion: a nonempty polyhedron has an extreme point iff it does not contain a line</li> </ol> <p>Def box \u2014 Polyhedron contains a line</p> <p>A polyhedron \\(P\\) contains a line if there exist some \\(x^0\\in P\\) and a nonzero direction \\(d\\neq 0\\) such that \\(x^0+\\lambda d\\in P\\) for every \\(\\lambda\\in\\mathbb{R}\\).</p> <p>(So you can move in both \\(+d\\) and \\(-d\\) directions forever and remain feasible.)</p> <p>Theorem box \u2014 Existence of extreme points</p> <p>Let \\(P\\) be a nonempty polyhedron. Then:</p> <p>\\(P\\) has at least one extreme point \\(\\Longleftrightarrow\\) \\(P\\) does not contain a line.</p> <p>Geometric meaning</p> <ul> <li>If \\(P\\) contains a line, it has a \u201cflat direction\u201d in both ways, so you cannot get a true corner.  </li> <li>If \\(P does not contain a line\\), then \\(P\\) has at least one corner (an extreme point / BFS).</li> </ul>"},{"location":"chapter2/notes2/#26-optimality-of-extreme-points","title":"2.6 Optimality of Extreme Points","text":"<p>Key box</p> <p>This section links geometry to optimization:</p> <ol> <li>Under mild conditions, if the LP is not unbounded, an optimal extreme point exists </li> <li>This is the geometric reason simplex can search over corners</li> </ol> <p>Theorem box \u2014 Optimality at an extreme point (when extreme points exist)</p> <p>Let \\(P\\) be a nonempty polyhedron that has at least one extreme point. Consider the LP \\(\\min\\{c^\\mathsf{T}x \\mid x\\in P\\}.\\)</p> <p>Then exactly one of the following holds:</p> <ol> <li>The optimal cost is \\(-\\infty\\) (the problem is unbounded below), or  </li> <li>There exists an optimal solution that is an extreme point of \\(P\\).</li> </ol> <p>Corollary box \u2014 Existence of an optimal solution (general nonempty polyhedron)</p> <p>Let \\(P\\) be a nonempty polyhedron and consider \\(\\min\\{c^\\mathsf{T}x \\mid x\\in P\\}\\). Then either:</p> <ol> <li>The optimal cost is \\(-\\infty\\), or  </li> <li>There exists at least one optimal solution (not necessarily unique).</li> </ol> <p>How to read this</p> <ul> <li>If a finite optimum is attained and \\(P\\) has extreme points, you can look for an optimal extreme point.  </li> <li>If \\(P\\) has no extreme points, the geometry is \u201cline-like,\u201d and the behavior is governed by that line direction.</li> </ul>"},{"location":"chapter2/notes2/#27-representation-of-bounded-polyhedra","title":"2.7 Representation of Bounded Polyhedra*","text":"<p>Key box</p> <p>This section gives a powerful \u201cvertex representation\u201d of bounded feasible sets:</p> <ol> <li>A bounded polyhedron (a polytope) is completely determined by its extreme points </li> <li>Every point in a bounded polyhedron can be written as a convex combination of extreme points  </li> <li>This gives the V-representation (by vertices) vs H-representation (by inequalities)</li> </ol> <p>Theorem box \u2014 Bounded polyhedron = convex hull of extreme points</p> <p>Let \\(P\\) be a nonempty bounded polyhedron. Let \\(V\\) be the set of extreme points of \\(P\\).</p> <p>Then: 1. \\(V\\) is finite, and 2. \\(P=\\mathrm{conv}(V)\\), i.e., every \\(x\\in P\\) can be written as a convex combination of extreme points:    \\(x=\\sum_{k=1}^K \\lambda_k v^k,\\)    where \\(v^k\\in V\\), \\(\\lambda_k\\ge 0\\), and \\(\\sum_{k=1}^K\\lambda_k=1\\).</p> <p>Meaning</p> <p>For bounded LP feasible regions, \u201cthe whole shape\u201d is exactly the convex hull of its corners.</p>"},{"location":"chapter2/notes2/#28-projections-of-polyhedra-fouriermotzkin-elimination","title":"2.8 Projections of Polyhedra: Fourier\u2013Motzkin Elimination","text":"<p>Key box</p> <p>This section explains how eliminating variables preserves polyhedral structure:</p> <ol> <li>Define coordinate projection \\(\\Pi_k(S)\\) </li> <li>Key fact: projection of a polyhedron is a polyhedron </li> <li>Provide an explicit elimination procedure: Fourier\u2013Motzkin elimination </li> <li>Note: constraint count can blow up (often exponentially across many eliminations)</li> </ol>"},{"location":"chapter2/notes2/#projection","title":"Projection","text":"<p>Def box \u2014 Projection map and projected set</p> <p>Let \\(\\pi_k:\\mathbb{R}^n\\to\\mathbb{R}^k\\) be the map that keeps the first \\(k\\) coordinates: \\(\\pi_k(x_1,\\dots,x_n)=(x_1,\\dots,x_k).\\)</p> <p>For a set \\(S\\subseteq\\mathbb{R}^n\\), its projection onto the first \\(k\\) coordinates is \\(\\Pi_k(S)=\\{\\pi_k(x)\\mid x\\in S\\}\\subseteq\\mathbb{R}^k.\\)</p> <p>Theorem box \u2014 Projection of a polyhedron is a polyhedron</p> <p>If \\(P\\subseteq\\mathbb{R}^n\\) is a polyhedron, then \\(\\Pi_k(P)\\subseteq\\mathbb{R}^k\\) is also a polyhedron.</p>"},{"location":"chapter2/notes2/#fouriermotzkin-elimination-eliminate-one-variable","title":"Fourier\u2013Motzkin elimination (eliminate one variable)","text":"<p>Suppose we have a system of inequalities in variables \\((x_1,\\dots,x_{n-1},x_n)\\) and we want to eliminate \\(x_n\\).</p> <p>Algorithm box \u2014 Fourier\u2013Motzkin elimination (eliminate \\(x_n\\))</p> <p>Start with inequalities of the form \\(a_i x_n + \\sum_{j=1}^{n-1} a_{ij}x_j \\ge b_i,\\quad i=1,\\dots,m.\\)</p> <ol> <li> <p>Split constraints into three groups by the coefficient of \\(x_n\\):</p> </li> <li> <p>\\(P=\\{i\\mid a_i&gt;0\\}\\) </p> </li> <li>\\(N=\\{i\\mid a_i&lt;0\\}\\) </li> <li> <p>\\(Z=\\{i\\mid a_i=0\\}\\)</p> </li> <li> <p>For \\(i\\in P\\), solve for a lower bound on \\(x_n\\):    \\(x_n \\ge \\dfrac{b_i-\\sum_{j=1}^{n-1}a_{ij}x_j}{a_i}.\\)</p> </li> <li> <p>For \\(i\\in N\\), solve for an upper bound on \\(x_n\\) (note inequality flips when dividing by \\(a_i&lt;0\\)):    \\(x_n \\le \\dfrac{b_i-\\sum_{j=1}^{n-1}a_{ij}x_j}{a_i}.\\)</p> </li> <li> <p>To eliminate \\(x_n\\), enforce that every lower bound is \\(\\le\\) every upper bound:    for each \\((i\\in P,\\ k\\in N)\\) impose    \\(\\dfrac{b_i-\\sum_{j=1}^{n-1}a_{ij}x_j}{a_i}    \\le    \\dfrac{b_k-\\sum_{j=1}^{n-1}a_{kj}x_j}{a_k}.\\)</p> </li> </ol> <p>These are inequalities involving only \\((x_1,\\dots,x_{n-1})\\).</p> <ol> <li>Keep all inequalities in \\(Z\\) unchanged (they already do not involve \\(x_n\\)).</li> </ol> <p>The resulting system describes the projection onto \\((x_1,\\dots,x_{n-1})\\).</p> <p>Complexity note</p> <p>One elimination step can create up to \\(|P|\\cdot|N|\\) new inequalities. Repeating across many variables may cause a rapid growth in the number of constraints.</p>"},{"location":"chapter2/notes2/#29-summary","title":"2.9 Summary","text":"<p>Key box</p> <p>Chapter 2 main takeaways:</p> <ol> <li>A polyhedron is an intersection of halfspaces and is convex </li> <li>Corner notions are equivalent: vertex = extreme point = BFS </li> <li>Degeneracy: a BFS can have more than \\(n\\) active constraints (extra zeros in standard form)  </li> <li>A nonempty polyhedron has an extreme point iff it does not contain a line  </li> <li>If the LP is not unbounded below and extreme points exist, an optimal extreme point exists </li> <li>Every nonempty bounded polyhedron is the convex hull of its extreme points </li> <li>Projections of polyhedra are polyhedra; Fourier\u2013Motzkin elimination performs projection by eliminating variables</li> </ol>"},{"location":"chapter3/","title":"Chapter 3","text":""},{"location":"chapter3/#notes","title":"Notes","text":"<ul> <li>Notes</li> </ul>"},{"location":"chapter3/#doubts","title":"Doubts","text":"<ul> <li>Doubts</li> </ul>"},{"location":"chapter3/doubts/","title":"Quick fixes for recurring confusions","text":""},{"location":"chapter3/doubts/#1-how-can-a-vector-x-have-a-rank-its-not-a-matrix","title":"1) \u201cHow can a vector \\(x\\) have a rank? It\u2019s not a matrix.\u201d","text":"<p>Fix: A vector doesn\u2019t have rank (in the matrix-rank sense). When notes say something like \u201crank of \\(x\\) is \\(k\\)\u201d, they almost always mean one of these:</p> <ul> <li>Rank of a matrix built from vectors, e.g., rank of \\([v_1\\ \\cdots\\ v_k]\\).</li> <li>Number of linearly independent active constraint normals at \\(x\\).</li> <li>Dimension of the affine hull of a set (sometimes loosely called \u201crank\u201d in informal lecture talk).</li> </ul> <p>If you show me the exact sentence again, we can map it to the correct interpretation.</p>"},{"location":"chapter3/doubts/#2-why-does-local-optimality-imply-global-optimality-in-lp","title":"2) \u201cWhy does local optimality imply global optimality in LP?\u201d","text":"<p>Because: - \\(P\\) is a convex set (intersection of a plane \\(Ax=b\\) with halfspaces \\(x\\ge 0\\)), and - the objective \\(c^\\top x\\) is convex (linear).</p> <p>So if there is no feasible direction with \\(c^\\top d&lt;0\\) at a feasible point, you are globally optimal.</p>"},{"location":"chapter3/doubts/#3-they-say-ctop-y-ge-v-and-ctop-z-ge-v-implies-ctop-yv-and-ctop-zv-why","title":"3) \u201cThey say \\(c^\\top y \\ge v\\) and \\(c^\\top z \\ge v\\) implies \\(c^\\top y=v\\) and \\(c^\\top z=v\\). Why?\u201d","text":"<p>That implication is not automatically true for arbitrary \\(y,z\\). It\u2019s true only if: - \\(v\\) is defined as the optimal value (the minimum of \\(c^\\top x\\) over the feasible set), and - \\(y\\) and \\(z\\) are both optimal solutions (i.e., they attain the minimum).</p> <p>Then by definition: $$ v \\le c^\\top y,\\quad v \\le c^\\top z, $$ and if \\(y,z\\) are optimal, equality holds: $$ c^\\top y=v,\\quad c^\\top z=v. $$</p>"},{"location":"chapter3/doubts/#4-what-do-we-gain-by-writing-ctop-yv-isnt-it-just-choosing-lambda1-so-yx","title":"4) \u201cWhat do we gain by writing \\(c^\\top y=v\\)? Isn\u2019t it just choosing \\(\\lambda=1\\) so \\(y=x^*\\)?\u201d","text":"<p>The gain is that \\(y\\) and \\(z\\) can be distinct optimal points. If the objective is flat along a face, there are infinitely many optimal points on that face. Showing multiple points achieve the same \\(v\\) is a key step in arguments about: - optimal faces, - extreme points, - expressing an optimal point as a convex combination of corners.</p>"},{"location":"chapter3/doubts/#5-how-does-x-is-a-cornerextreme-point-solve-a-contradiction","title":"5) \u201cHow does \u2018\\(x^*\\) is a corner/extreme point\u2019 solve a contradiction?\u201d","text":"<p>Typical structure of those proofs: - Assume an optimal solution is not at a corner. - Then you can write it as a strict convex combination of two distinct feasible points. - Because the objective is linear, the objective value at the combination is the same convex combination of objective values. - If the combination is optimal, that forces the endpoints to be optimal too. - This contradicts \u201cnot a corner\u201d (or forces a descent direction), depending on the theorem.</p> <p>So \u201ccorner\u201d matters because corners cannot be decomposed nontrivially into two different feasible points.</p>"},{"location":"chapter3/doubts/#6-if-an-lp-has-m-constraints-and-n-variables-what-does-rank-tell-us-about-existence","title":"6) \u201cIf an LP has \\(m\\) constraints and \\(n\\) variables, what does rank tell us about existence?\u201d","text":"<p>Careful: there are multiple \u201csystems\u201d here.</p> <ul> <li>For equality constraints \\(Ax=b\\), feasibility depends on whether \\(b\\) lies in the column space of \\(A\\).  </li> <li>If \\(\\mathrm{rank}(A)&lt;m\\), it does not automatically mean infeasible. It means some rows are dependent (constraints redundant or inconsistent depending on \\(b\\)).  </li> <li> <p>In general: feasibility \u21d4 \\(Ax=b\\) is consistent.</p> </li> <li> <p>For standard form \\(Ax=b, x\\ge 0\\), feasibility is stricter: even if \\(Ax=b\\) is consistent, you may have no nonnegative solution.</p> </li> </ul>"},{"location":"chapter3/doubts/#7-set-s-is-convex-what-exactly-does-that-mean","title":"7) \u201cSet \\(S\\) is convex \u2014 what exactly does that mean?\u201d","text":"<p>Definition: $$ S \\text{ convex } \\iff \\forall x,y\\in S,\\ \\forall \\lambda\\in[0,1],\\ \\lambda x+(1-\\lambda)y\\in S. $$ Geometric meaning: line segment between any two points in \\(S\\) stays inside \\(S\\).</p>"},{"location":"chapter3/doubts/#8-why-do-we-introduce-reduced-costs-in-simplex","title":"8) \u201cWhy do we introduce reduced costs in simplex?\u201d","text":"<p>Reduced cost \\(\\bar c_j\\) is the instantaneous rate of objective change when we try to increase a nonbasic variable \\(x_j\\) from zero while keeping feasibility: $$ \\bar c_j = c_j - c_B^\\top B^{-1}A_j. $$ - If \\(\\bar c_j &lt; 0\\) and the BFS is nondegenerate, then moving in that basic direction decreases cost (profitable entering variable). - If all \\(\\bar c_j \\ge 0\\), no edge from that BFS improves the cost \u21d2 optimal (nondegenerate case).</p>"},{"location":"chapter3/doubts/#add-more-doubts-as-we-go","title":"Add more doubts as we go","text":"<p>Whenever you ask a question that feels like a \u201crecurring confusion,\u201d I\u2019ll append it here with the cleanest explanation + any small example we used.</p>"},{"location":"chapter3/doubts/#11-what-is-cycling-in-simplex-and-how-do-anticycling-rules-fix-it","title":"11) \u201cWhat is cycling in simplex and how do anticycling rules fix it?\u201d","text":"<p>Cycling happens only in degenerate LPs.</p> <ul> <li>In a degenerate BFS, the ratio test can give \\(\\theta^*=0\\).</li> <li>Then the basis changes but the point \\(x\\) does not move.</li> <li>After several such zero-length pivots, simplex can return to an earlier basis and repeat forever.</li> </ul> <p>Anticycling rules guarantee that no basis repeats, so the method must terminate.</p> <p>Two standard anticycling rules in this chapter: - Lexicographic pivoting: break ratio-test ties by comparing normalized tableau rows lexicographically; this makes the objective row increase lexicographically each pivot, so repetition is impossible. - Bland\u2019s rule: choose the entering variable with smallest index, and among tied leaving choices choose the smallest index; this also prevents cycling.</p>"},{"location":"chapter3/doubts/#12-what-is-the-auxiliary-variable-phase-i-method-and-why-minimize-sum-y_i","title":"12) \u201cWhat is the auxiliary-variable (Phase I) method, and why minimize \\(\\sum y_i\\)?\u201d","text":"<p>When the LP is in standard form $$ Ax=b,\\quad x\\ge 0, $$ we may not have an obvious starting BFS. We introduce artificial variables \\(y\\ge 0\\) and solve:</p> \\[ \\min\\ \\sum_{i=1}^m y_i \\quad \\text{s.t.}\\quad Ax+y=b,\\; x\\ge 0,\\ y\\ge 0. \\] <ul> <li>This auxiliary LP always has an easy BFS: \\(x=0\\), \\(y=b\\) (assuming \\(b\\ge 0\\)).</li> <li>If the original system has a feasible \\(x\\ge 0\\) with \\(Ax=b\\), then \\((x,0)\\) is feasible for the auxiliary LP and gives objective 0.</li> <li>Because \\(\\sum y_i\\ge 0\\), the auxiliary optimum is 0 iff the original problem is feasible.</li> </ul> <p>So Phase I is both a feasibility test and a way to produce a feasible starting basis for Phase II.</p>"},{"location":"chapter3/doubts/#13-phase-i-ended-with-an-artificial-variable-still-basic-value-0-what-do-we-do","title":"13) \u201cPhase I ended with an artificial variable still basic (value 0). What do we do?\u201d","text":"<p>This happens under degeneracy. If an artificial variable \\(y_\\ell\\) is basic but equals 0 at the end of Phase I:</p> <ul> <li>Look at the corresponding row (the \\(\\ell\\)-th row) of \\(B^{-1}A\\).</li> <li>If all entries in that row (for original columns) are 0, the equality constraint is redundant, so you can drop that row.</li> <li>Otherwise, pick a column with a nonzero entry in that row and pivot so that an original variable enters and the artificial variable leaves.</li> </ul> <p>Repeat until all artificial variables are out of the basis.</p>"},{"location":"chapter3/doubts/#14-big-m-vs-two-phase-simplex-whats-the-difference","title":"14) \u201cBig-\\(M\\) vs two-phase simplex \u2014 what\u2019s the difference?\u201d","text":"<ul> <li>Two-phase simplex: solve Phase I (minimize \\(\\sum y_i\\)) to get feasibility; then Phase II solves the original objective starting from that feasible basis.</li> <li>Big-\\(M\\): solve one LP with objective \\(c^\\top x + M\\sum y_i\\) where \\(M\\) is huge, so the method prioritizes driving \\(y\\) to zero.</li> </ul> <p>In practice, two-phase is often preferred for numerical stability (big-\\(M\\) can require dangerously large constants in floating point arithmetic).</p>"},{"location":"chapter3/doubts/#15-why-do-they-add-the-convexity-constraint-etop-x1-in-the-geometry-section","title":"15) \u201cWhy do they add the convexity constraint \\(e^\\top x=1\\) in the geometry section?\u201d","text":"<p>With \\(x\\ge 0\\) and \\(e^\\top x=1\\), the vector \\(x\\) becomes a set of convex combination weights.</p> <p>Then: $$ b=Ax=\\sum_i x_i A_i $$ means \\(b\\) lies in the convex hull of the columns \\(A_i\\), and $$ z=c^\\top x=\\sum_i x_i c_i $$ means \\((b,z)\\) lies in the convex hull of the lifted points \\((A_i,c_i)\\).</p> <p>That makes the geometry clean: feasibility becomes \u201cdoes the vertical requirement line through \\(b\\) intersect the convex hull?\u201d The book notes that any bounded LP can be transformed into this form (exercise), so this picture still has general meaning.</p>"},{"location":"chapter3/doubts/#16-what-is-the-requirement-line","title":"16) \u201cWhat is the requirement line?\u201d","text":"<p>It is the vertical line in \\((m+1)\\)-dimensional space: $$ {(b,z)\\mid z\\in\\mathbb{R}}, $$ where \\(b\\) is fixed by the constraints \\(Ax=b\\). Feasible solutions correspond to intersection points of this line with the convex hull of \\((A_i,c_i)\\), and the optimal solution is the lowest intersection point.</p>"},{"location":"chapter3/doubts/#17-what-is-the-dual-plane-and-why-is-below-the-plane-the-same-as-negative-reduced-cost","title":"17) \u201cWhat is the dual plane, and why is \u2018below the plane\u2019 the same as negative reduced cost?\u201d","text":"<p>The basic points \\((A_{B(i)},c_{B(i)})\\) lie on an \\(m\\)-dimensional hyperplane (dual plane). A candidate point \\((A_j,c_j)\\) lying below that plane means that swapping it into the basis can lower the intersection height on the requirement line.</p> <p>Algebraically, \u201cbelow the plane\u201d is equivalent to \\(\\bar c_j&lt;0\\) (for minimization), so reduced cost is a signed vertical distance from the dual plane.</p>"},{"location":"chapter3/doubts/#18-how-can-simplex-be-fast-in-practice-if-worst-case-is-exponential","title":"18) \u201cHow can simplex be fast in practice if worst-case is exponential?\u201d","text":"<p>Worst-case examples force simplex to walk an exponentially long path of vertices. But typical real LPs have structure and good pivot rules often reach optimality in far fewer pivots.</p> <p>Theory section 3.7 separates: - per-iteration work (handled by revised simplex / sparsity), and - number of iterations (can be exponential in worst case).</p> <p>Average-case behavior depends on the probabilistic model for \u201crandom LP,\u201d so it\u2019s harder to state universal guarantees.</p>"},{"location":"chapter3/notes/","title":"Chapter 3 \u2014 The Simplex Method","text":""},{"location":"chapter3/notes/#31-optimality-conditions","title":"3.1 Optimality conditions","text":"<p>We consider the standard form linear program:</p> \\[ \\min \\; c^\\top x \\quad \\text{s.t.}\\quad Ax=b,\\; x\\ge 0, \\] <ul> <li>\\(A\\) is \\(m\\times n\\) and its rows are linearly independent (rank \\(m\\)).  </li> <li>\\(P=\\{x\\in\\mathbb{R}^n\\mid Ax=b,\\;x\\ge 0\\}\\) is the feasible set.  </li> <li>\\(A_i\\) denotes the i-th column of \\(A\\); \\(a_i^\\top\\) denotes the i-th row of \\(A\\).</li> </ul>"},{"location":"chapter3/notes/#311-local-improvement-is-enough-in-lp","title":"3.1.1 Local improvement is enough in LP","text":"<p>Many algorithms try to improve the objective by searching \u201cnearby\u201d feasible points. For general nonconvex problems, \u201cno improving nearby move\u201d only implies local optimality.  </p> <p>For LP, local optimality implies global optimality, because: - the objective \\(c^\\top x\\) is convex (actually linear), and - the feasible set \\(P\\) is convex.</p> <p>So if, at a feasible point, you cannot move in any feasible direction that decreases cost, that point is optimal.</p>"},{"location":"chapter3/notes/#312-feasible-directions","title":"3.1.2 Feasible directions","text":"<p>Definition (Feasible direction at \\(x\\)) A vector \\(d\\in\\mathbb{R}^n\\) is a feasible direction at \\(x\\in P\\) if there exists \\(\\varepsilon&gt;0\\) such that $$ x+\\theta d \\in P \\quad \\text{for all } \\theta\\in[0,\\varepsilon]. $$</p> <p>Key constraints for a feasible direction - Equality constraints: since \\(Ax=b\\), moving must keep \\(Ax\\) unchanged:   $$   A(x+\\theta d)=b\\ \\forall \\theta \\ \\Rightarrow\\ Ad=0.   $$ - Nonnegativity: for each component where \\(x_i=0\\), you cannot decrease it:   $$   x_i=0 \\ \\Rightarrow\\ d_i\\ge 0.   $$ (That second condition is formalized later as an exercise in the book; it\u2019s the clean algebraic description of Figure 3.1.)</p>"},{"location":"chapter3/notes/#313-basic-feasible-solutions-and-basic-directions","title":"3.1.3 Basic feasible solutions and \u201cbasic directions\u201d","text":"<p>Let \\(x\\) be a basic feasible solution (BFS). That means: - There is an index set of basic variables \\(B=\\{B(1),\\dots,B(m)\\}\\). - The corresponding basis matrix   $$   B=[A_{B(1)}\\ \\cdots\\ A_{B(m)}]   $$   is invertible. - All nonbasic variables are at zero: \\(x_j=0\\) for \\(j\\notin B\\). - The basic variables satisfy:   $$   x_B = B^{-1}b.   $$</p> <p>Now pick a nonbasic index \\(j\\notin B\\). A natural \u201cedge move\u201d from a BFS is: increase \\(x_j\\) from 0, while keeping all other nonbasic variables at 0.</p> <p>Construct a direction \\(d\\) by: - \\(d_j=1\\), - \\(d_i=0\\) for all other nonbasic \\(i\\neq j\\), - choose \\(d_B\\) so that \\(Ad=0\\).</p> <p>Since \\(Ad = Bd_B + A_j = 0\\), we get the basic-direction formula: $$ d_B = -B^{-1}A_j. $$ This is equation (3.1) in the text.</p> <p>Intuition: increasing \\(x_j\\) forces a compensating change in the basic variables so that \\(Ax=b\\) remains true.</p>"},{"location":"chapter3/notes/#314-degeneracy-a-basic-direction-might-not-be-feasible","title":"3.1.4 Degeneracy: a \u201cbasic direction\u201d might not be feasible","text":"<p>Even if \\(Ad=0\\), the move can violate \\(x\\ge 0\\).</p> <ul> <li> <p>Nondegenerate BFS: \\(x_B&gt;0\\).   Then for small enough \\(\\theta&gt;0\\), \\(x_B+\\theta d_B\\ge 0\\) automatically, so the basic direction is feasible.</p> </li> <li> <p>Degenerate BFS: some basic variable is 0.   If that zero basic variable has a negative component in \\(d_B\\), then \\(x_B+\\theta d_B\\) becomes negative immediately, so that basic direction is not feasible.   (This is what Figure 3.2 illustrates.)</p> </li> </ul> <p>Subtle point: Degeneracy breaks the \u201ceasy test\u201d that \u201cnegative reduced cost \u21d2 there is a feasible improving edge,\u201d because the edge might point out of \\(P\\).</p>"},{"location":"chapter3/notes/#315-reduced-costs","title":"3.1.5 Reduced costs","text":"<p>Suppose we move along the \\(j\\)-th basic direction \\(d\\). The objective change rate is:</p> \\[ c^\\top d = c_B^\\top d_B + c_j. \\] <p>Using \\(d_B=-B^{-1}A_j\\),</p> \\[ c^\\top d = c_j - c_B^\\top B^{-1}A_j. \\] <p>This motivates:</p> <p>Definition (Reduced cost of variable \\(x_j\\)) $$ \\bar c_j \\;=\\; c_j - c_B^\\top B^{-1}A_j. $$</p> <p>Interpretation: - \\(c_j\\): direct cost per unit increase in \\(x_j\\). - \\(-c_B^\\top B^{-1}A_j\\): induced cost from adjusting the basic variables to keep feasibility.</p> <p>Important fact: reduced costs of basic variables are zero. Reason: if \\(j=B(i)\\) is basic, then \\(B^{-1}A_{B(i)}=e_i\\) (the \\(i\\)-th unit vector), so: $$ \\bar c_{B(i)} = c_{B(i)} - c_B^\\top e_i = c_{B(i)}-c_{B(i)}=0. $$</p>"},{"location":"chapter3/notes/#316-example-as-in-the-books-example-31","title":"3.1.6 Example (as in the book\u2019s Example 3.1)","text":"<p>For the LP:</p> \\[ \\min\\ c_1x_1+c_2x_2+c_3x_3+c_4x_4 $$ $$ \\text{s.t.}\\quad  x_1+x_2+x_3+x_4=2,\\quad 2x_1+3x_3+4x_4=2,\\quad x\\ge 0, \\] <p>Choose \\(x_1,x_2\\) basic. Then:</p> <p>$$ B=\\begin{bmatrix}1&amp;1\\2&amp;0\\end{bmatrix},\\qquad x_3=x_4=0. $$ Solve \\(Bx_B=b\\) to get \\(x_1=1,\\ x_2=1\\) (nondegenerate).</p> <p>For entering \\(x_3\\): \\(A_3=(1,3)^\\top\\). Direction: $$ d_3=1,\\ d_4=0,\\ d_B=-B^{-1}A_3. $$ The objective rate along this direction equals the reduced cost \\(\\bar c_3\\).</p>"},{"location":"chapter3/notes/#317-optimality-conditions-nondegenerate-case","title":"3.1.7 Optimality conditions (nondegenerate case)","text":"<p>Theorem (Optimality conditions via reduced costs) Let \\(x\\) be a basic feasible solution associated with a basis \\(B\\), and let \\(\\bar c\\) be the vector of reduced costs.</p> <ol> <li>If \\(\\bar c\\ge 0\\), then \\(x\\) is optimal.  </li> <li>If \\(x\\) is optimal and nondegenerate, then \\(\\bar c\\ge 0\\).</li> </ol> <p>Why it makes sense - \\(\\bar c_j\\) is the \u201cslope\u201d of the objective along the \\(j\\)-th basic direction. - If every slope is \\(\\ge 0\\), then none of the edge directions improves the cost, so \\(x\\) is optimal. - If \\(x\\) is nondegenerate and some \\(\\bar c_j&lt;0\\), then that direction is feasible and decreases cost \u21d2 \\(x\\) cannot be optimal.</p> <p>Subtle but crucial warning (degeneracy): An optimal degenerate BFS may still have some negative reduced costs, because the corresponding basic directions might not be feasible.</p>"},{"location":"chapter3/notes/#34-anticycling-lexicography-and-blands-rule","title":"3.4 Anticycling: lexicography and Bland\u2019s rule","text":""},{"location":"chapter3/notes/#341-why-cycling-happens-degeneracy","title":"3.4.1 Why cycling happens (degeneracy)","text":"<p>In the nondegenerate case, every pivot gives \\(\\theta^*&gt;0\\), so the objective value strictly decreases and no basis can repeat (Theorem 3.3).</p> <p>In the degenerate case, it can happen that the ratio test gives \\(\\theta^*=0\\). Then: - the basis changes, but - the basic feasible solution (the point \\(x\\)) does not change, and - the objective value does not improve.</p> <p>After a sequence of such \u201czero-length pivots,\u201d the algorithm can return to a previous basis and then repeat forever. This infinite loop is called cycling.</p> <p>So, in degenerate problems, we need pivoting rules that guarantee: - no basis is repeated, hence - termination in finitely many pivots.</p> <p>A key corollary of any anticycling rule is:</p> <p>If the optimal cost is finite, then there exists an optimal basis, i.e. a basis with \\(B^{-1}b\\ge 0\\) and reduced costs \\(\\bar c = c^\\top - c_B^\\top B^{-1}A \\ge 0\\).</p>"},{"location":"chapter3/notes/#342-lexicographic-order-definitions","title":"3.4.2 Lexicographic order (definitions)","text":"<p>Let \\(u,v\\in\\mathbb{R}^k\\).</p> <ul> <li>We say \\(u\\) is lexicographically smaller than \\(v\\) (write \\(u \\prec v\\)) if, at the first index where they differ,   the component of \\(u\\) is smaller:</li> <li>find the smallest \\(t\\) such that \\(u_t\\ne v_t\\),</li> <li> <p>then \\(u\\prec v\\) iff \\(u_t &lt; v_t\\).</p> </li> <li> <p>We say \\(u\\) is lexicographically positive if \\(u\\succ 0\\) (i.e., the first nonzero component of \\(u\\) is positive).</p> </li> </ul> <p>Example comparisons (same style as in the book): - \\((0,2,3,0) \\succ (0,2,1,4)\\) because at the first difference (third component), \\(3&gt;1\\). - \\((0,4,5,0) \\prec (1,2,1,2)\\) because at the first component, \\(0&lt;1\\).</p>"},{"location":"chapter3/notes/#343-the-lexicographic-pivoting-rule-full-tableau-form","title":"3.4.3 The lexicographic pivoting rule (full tableau form)","text":"<p>Assume we are using a full tableau (so each basic row is visible).</p> <p>Fix an entering column \\(j\\). Let the pivot column entries in the constraint rows be $$ u_i = (B^{-1}A_j)_i. $$</p> <p>Among rows with \\(u_i&gt;0\\), the usual ratio test looks at \\(\\dfrac{x_{B(i)}}{u_i}\\) and chooses the minimum. If there is a tie, the lexicographic rule breaks ties as follows:</p> <p>Lexicographic leaving rule: choose the pivot row \\(\\ell\\) such that \\(u_\\ell&gt;0\\) and $$ \\frac{\\text{row }\\ell}{u_\\ell} \\prec \\frac{\\text{row }i}{u_i} \\quad \\text{for every } i\\ne \\ell \\text{ with } u_i&gt;0. \\tag{3.5} $$</p> <p>In words: divide each candidate row by its pivot-column entry, then pick the row whose normalized row is lexicographically smallest.</p>"},{"location":"chapter3/notes/#344-example-37-how-tie-breaking-works","title":"3.4.4 Example 3.7 (how tie-breaking works)","text":"<p>Suppose the pivot column is \\(j=3\\).</p> <p>Assume two rows tie in the ratio test: $$ \\frac{x_{B(1)}}{u_1} = \\frac{1}{3}, \\qquad \\frac{x_{B(3)}}{u_3} = \\frac{3}{9} = \\frac{1}{3}. $$</p> <p>So we divide the candidate rows by their pivot entries: - row 1 divided by \\(u_1=3\\), - row 3 divided by \\(u_3=9\\),</p> <p>and then compare the normalized rows lexicographically. If the comparison yields (as in the book) something like $$ \\frac{7}{9} &lt; \\frac{5}{3}, $$ then row 3 is lexicographically smaller, so row 3 leaves.</p> <p>Important subtle point: under the standing assumption that \\(A\\) has linearly independent rows, this rule leads to a unique leaving row. Reason (book\u2019s argument): if two candidate normalized rows were identical, two rows of \\(B^{-1}A\\) would be proportional, forcing \\(\\mathrm{rank}(B^{-1}A)&lt;m\\), hence \\(\\mathrm{rank}(A)&lt;m\\), contradicting independence of rows of \\(A\\).</p>"},{"location":"chapter3/notes/#345-why-lexicographic-pivoting-prevents-cycling-main-idea","title":"3.4.5 Why lexicographic pivoting prevents cycling (main idea)","text":"<p>The proof uses three facts (book\u2019s structure):</p> <ol> <li>If constraint rows start lexicographically positive, they stay lexicographically positive. </li> <li>Pivot row is divided by a positive number \u21d2 stays lex-positive.  </li> <li>For any other row with \\(u_i&lt;0\\), we add a positive multiple of the pivot row to eliminate the pivot entry \u21d2 lex-positive stays lex-positive.  </li> <li> <p>For \\(u_i&gt;0\\) (and \\(i\\ne \\ell\\)), the lexicographic choice (3.5) guarantees the update keeps the row lex-positive.</p> </li> <li> <p>The objective (zeroth) row increases lexicographically at each pivot.    At the start of an iteration, the reduced cost in the entering column is negative.    To make it zero, we add a positive multiple of the pivot row to the objective row.    Since the pivot row is lexicographically positive, the objective row becomes lexicographically larger.</p> </li> <li> <p>Therefore, no tableau repeats, hence no basis repeats.    Because the objective row strictly increases lexicographically each iteration, it cannot return to a previous value.    But for a fixed basis, the tableau (and objective row) is uniquely determined.    So the basis can never repeat \u21d2 no cycling \u21d2 termination.</p> </li> </ol>"},{"location":"chapter3/notes/#346-blands-rule-smallest-subscript-pivoting","title":"3.4.6 Bland\u2019s rule (smallest-subscript pivoting)","text":"<p>Lexicographic pivoting is clean in a full tableau setting, but it is not very convenient in sophisticated revised simplex codes (where \\(B^{-1}\\) is not explicitly formed).</p> <p>A simpler anticycling rule is Bland\u2019s rule:</p> <p>Bland\u2019s entering rule: among all eligible entering variables (for minimization: \\(\\bar c_j&lt;0\\)), choose the one with the smallest index \\(j\\).</p> <p>Bland\u2019s leaving rule: apply the ratio test; among all ties for the minimum ratio, choose the leaving basic variable with the smallest index.</p> <p>Theorem (Bland). If Bland\u2019s rule is used, the simplex method cannot cycle and must terminate in finitely many pivots.</p> <p>Practical meaning: - Bland\u2019s rule is easy to implement in revised simplex. - It may take more iterations than aggressive pricing rules, but it gives a theoretical guarantee.</p>"},{"location":"chapter3/notes/#347-practical-notes","title":"3.4.7 Practical notes","text":"<ul> <li> <p>Lexicographic pivoting is naturally expressed with a full tableau (where you can literally compare rows).   It can be used with revised simplex only if you maintain enough tableau-like information (e.g., explicit \\(B^{-1}\\)), which many modern sparse implementations avoid.</p> </li> <li> <p>Bland\u2019s rule is the simplest \u201calways safe\u201d fallback when degeneracy causes stalling/cycling concerns.</p> </li> </ul>"},{"location":"chapter3/notes/#35-finding-an-initial-basic-feasible-solution-initial-bfs","title":"3.5 Finding an initial basic feasible solution (initial BFS)","text":""},{"location":"chapter3/notes/#351-the-problem-simplex-needs-a-starting-bfs","title":"3.5.1 The problem: simplex needs a starting BFS","text":"<p>The simplex method (Sections 3.2\u20133.4) assumes we start from a basic feasible solution (BFS): - choose a basis matrix \\(B\\) (invertible), - set \\(x_N=0\\), - compute \\(x_B=B^{-1}b\\), - and require \\(x_B\\ge 0\\).</p> <p>Sometimes a BFS is obvious; often it is not.</p>"},{"location":"chapter3/notes/#352-easy-case-inequalities-with-nonnegative-right-hand-side","title":"3.5.2 Easy case: inequalities with nonnegative right-hand side","text":"<p>If the original constraints are of the form $$ Ax \\le b, \\quad b\\ge 0, $$ introduce slack variables \\(s\\ge 0\\): $$ Ax+s=b. $$</p> <p>Then the point - \\(x=0\\), - \\(s=b\\),</p> <p>is feasible, and the basis matrix is the identity (columns of \\(s\\)). So we immediately have a BFS.</p>"},{"location":"chapter3/notes/#353-general-case-use-an-auxiliary-phase-i-problem","title":"3.5.3 General case: use an auxiliary (Phase I) problem","text":"<p>Now consider standard form $$ \\min\\ c^\\top x \\quad \\text{s.t.}\\quad Ax=b,\\; x\\ge 0, $$ and assume (after possibly multiplying some equalities by \\(-1\\)) that $$ b\\ge 0. $$</p> <p>If there is no obvious BFS, we introduce artificial variables $$ y\\in\\mathbb{R}^m,\\quad y\\ge 0, $$ and solve the auxiliary problem (Phase I):</p> \\[ \\min\\ \\sum_{i=1}^m y_i \\quad \\text{s.t.}\\quad Ax + y = b,\\; x\\ge 0,\\; y\\ge 0. \\tag{Phase I} \\]"},{"location":"chapter3/notes/#why-this-works-the-key-logic","title":"Why this works (the key logic)","text":"<p>1) Phase I is always easy to initialize. Set \\(x=0\\) and \\(y=b\\). Then \\(Ax+y=b\\) holds and \\(y\\ge 0\\). The basis matrix is the identity (columns of \\(y\\)), so this is a BFS.</p> <p>2) If the original problem is feasible, Phase I optimum is 0. If there exists some \\(x\\ge 0\\) with \\(Ax=b\\), then choosing that \\(x\\) and \\(y=0\\) is feasible for Phase I and gives objective 0. Since \\(\\sum y_i\\ge 0\\) always, the optimal Phase I value must be 0.</p> <p>3) If Phase I optimum is positive, the original problem is infeasible. Because we could not drive all \\(y_i\\) to zero, there is no \\(x\\ge 0\\) that satisfies \\(Ax=b\\).</p> <p>So Phase I is a feasibility detector and also provides a feasible point for the original LP when feasible.</p>"},{"location":"chapter3/notes/#354-the-two-phase-simplex-method-algorithm-form","title":"3.5.4 The two-phase simplex method (algorithm form)","text":"<p>Phase I 1. Multiply some equalities by \\(-1\\) so that \\(b\\ge 0\\). 2. Add artificial variables \\(y_1,\\dots,y_m\\) and solve the auxiliary LP    $$    \\min\\ \\sum_{i=1}^m y_i    \\quad \\text{s.t.}\\quad    Ax+y=b,\\; x\\ge 0,\\; y\\ge 0.    $$ 3. If the optimal value is \\(&gt;0\\), stop: the original LP is infeasible. 4. If the optimal value is \\(0\\), we have found a feasible solution with \\(y=0\\).    - If no artificial variable is basic in the final basis, drop the \\(y\\)-columns and start Phase II.    - If some artificial variable is still basic (necessarily with value 0), proceed to Step 5. 5. Drive artificial variables out of the basis (or remove redundant constraints):    - Let the \\(\\ell\\)-th basic variable be artificial (\\(y_\\ell\\)).    - Look at the \\(\\ell\\)-th row of the current tableau (equivalently the \\(\\ell\\)-th row of \\(B^{-1}A\\)).      - If all entries in that row for the original columns are zero, then that equality is redundant and can be eliminated (drop the row).      - Otherwise, pick a column \\(A_j\\) whose entry in that row is nonzero and pivot so that \\(x_j\\) enters and \\(y_\\ell\\) leaves.    - Repeat until all artificial variables are nonbasic.</p> <p>Phase II 1. Use the final basis from Phase I (containing only columns of \\(A\\)) as the starting basis. 2. Recompute reduced costs using the original cost vector \\(c\\). 3. Run simplex on the original LP.</p>"},{"location":"chapter3/notes/#355-a-complete-worked-example-two-phase-full-tableau-style","title":"3.5.5 A complete worked example (two-phase, full tableau style)","text":"<p>Consider the LP $$ \\min\\ z = x_1 + 2x_2 $$ subject to $$ \\begin{aligned} x_1 - x_2 &amp;= 1,\\ x_1 + x_2 + x_3 &amp;= 3,\\ x_1,x_2,x_3 &amp;\\ge 0. \\end{aligned} $$</p> <p>There is no obvious BFS for \\(Ax=b\\) (you cannot set \\(x=0\\)).</p>"},{"location":"chapter3/notes/#phase-i-add-artificial-variables","title":"Phase I: add artificial variables","text":"<p>Introduce \\(y_1,y_2\\ge 0\\): $$ \\begin{aligned} x_1 - x_2 + y_1 &amp;= 1,\\ x_1 + x_2 + x_3 + y_2 &amp;= 3. \\end{aligned} $$ Phase I objective: $$ \\min\\ w = y_1+y_2. $$</p> <p>Start BFS: \\(x=0\\), \\(y=b=(1,3)\\).</p> <p>Running tableau pivots (as shown earlier in chat), Phase I ends at $$ (x_1,x_2,x_3,y_1,y_2)=(2,1,0,0,0), $$ so \\(w^*=0\\) and the original constraints are feasible.</p>"},{"location":"chapter3/notes/#phase-ii-drop-y-and-optimize-original-z","title":"Phase II: drop \\(y\\) and optimize original \\(z\\)","text":"<p>Using the Phase I final basis as the starting basis, Phase II yields the optimal solution $$ (x_1,x_2,x_3)=(1,0,2),\\qquad z^*=1. $$</p> <p>What to remember from this example - Phase I is only about forcing \\(y\\to 0\\) (feasibility). - Phase II reuses the found feasible basis and then optimizes the true cost.</p>"},{"location":"chapter3/notes/#356-the-big-m-method-single-phase-alternative","title":"3.5.6 The big-\\(M\\) method (single-phase alternative)","text":"<p>Instead of two phases, we combine them by penalizing artificial variables in the objective:</p> \\[ \\min\\ \\sum_{j=1}^n c_j x_j + M\\sum_{i=1}^m y_i, \\] <p>subject to $$ Ax+y=b,\\quad x\\ge 0,\\ y\\ge 0, $$ where \\(M\\) is a \u201cvery large\u201d positive constant.</p> <p>Meaning - The term \\(M\\sum y_i\\) makes it extremely expensive to keep \\(y_i&gt;0\\). - If the original problem is feasible and has a finite optimum, the algorithm drives \\(y\\to 0\\) and then behaves like minimizing \\(c^\\top x\\).</p> <p>Important implementation idea (used in the book): Treat \\(M\\) symbolically as a parameter and compare expressions like \\(-8M+1\\) as negative for \u201c\\(M\\) large enough,\u201d without fixing a numeric value.</p>"},{"location":"chapter3/notes/#357-comparison-two-phase-vs-big-m","title":"3.5.7 Comparison: two-phase vs big-\\(M\\)","text":"<p>Two-phase simplex: - Clean separation: Phase I finds feasibility, Phase II optimizes. - Numerically stable and standard in solvers.</p> <p>Big-\\(M\\): - One phase, simpler conceptually. - Can be numerically troublesome if \\(M\\) is taken as a huge number in floating point arithmetic. - Using symbolic \\(M\\) avoids choosing a specific value but is mainly a textbook device.</p>"},{"location":"chapter3/notes/#36-the-geometry-of-the-simplex-method-column-geometry","title":"3.6 The geometry of the simplex method (column geometry)","text":"<p>This section gives a geometric interpretation of simplex that explains (i) what a basis means geometrically, and (ii) why reduced costs control the choice of entering variables.</p>"},{"location":"chapter3/notes/#361-add-a-convexity-constraint","title":"3.6.1 Add a convexity constraint","text":"<p>The book considers the problem</p> \\[ \\min\\ c^\\top x \\quad \\text{s.t.}\\quad Ax=b,\\; e^\\top x=1,\\; x\\ge 0, \\tag{3.6} \\] <p>where \\(e\\) is the all-ones vector.</p> <ul> <li>The constraint \\(e^\\top x=1\\) is called the convexity constraint.</li> <li>Although it looks restrictive, the book notes that any bounded feasible set can be transformed into this form (Exercise 3.28).   The point is: the geometry they build is easiest to see when \\(x\\) is a convex combination.</li> </ul> <p>Define an auxiliary scalar variable $$ z=c^\\top x. $$</p>"},{"location":"chapter3/notes/#362-lift-the-columns-into-one-higher-dimension","title":"3.6.2 Lift the columns into one higher dimension","text":"<p>Let \\(A_1,\\dots,A_n\\) be the columns of \\(A\\) and \\(c_1,\\dots,c_n\\) be the cost coefficients.</p> <p>Because \\(x\\ge 0\\) and \\(\\sum_i x_i=1\\), $$ Ax = \\sum_{i=1}^n x_i A_i = b, $$ so \\(b\\) is a convex combination of the column vectors \\(A_i\\).</p> <p>Also $$ z=c^\\top x = \\sum_{i=1}^n x_i c_i. $$</p> <p>So the pair \\((b,z)\\) is a convex combination of the lifted points $$ (A_i,c_i)\\in\\mathbb{R}^{m+1}. $$</p> <p>This is the column geometry picture: - Horizontal space: \\(\\mathbb{R}^m\\) containing the columns \\(A_i\\) and the point \\(b\\). - Vertical axis: the cost coordinate \\(z\\).</p> <p>Each feasible solution corresponds to a point \\((b,z)\\) on a vertical line above \\(b\\).</p>"},{"location":"chapter3/notes/#363-requirement-line-and-feasibility","title":"3.6.3 Requirement line and feasibility","text":"<p>The vertical line through \\((b,\\cdot)\\) is the requirement line.</p> <p>Let \\(H\\) be the convex hull of the lifted points: $$ H = \\mathrm{conv}{(A_1,c_1),\\dots,(A_n,c_n)}. $$</p> <ul> <li>The LP is feasible iff the requirement line intersects \\(H\\).</li> <li>Among all intersection points, the optimal solution corresponds to the lowest point on that intersection (smallest \\(z\\)).</li> </ul> <p>So:</p> <p>Feasibility: requirement line intersects \\(H\\). Optimality: lowest intersection point.</p> <p>(This is exactly what Figure 3.6 is showing.)</p>"},{"location":"chapter3/notes/#364-affine-independence-and-simplices-definition-36","title":"3.6.4 Affine independence and simplices (Definition 3.6)","text":"<p>A collection of vectors \\(y^1,\\dots,y^{k+1}\\in\\mathbb{R}^n\\) is affinely independent if $$ y^1-y^{k+1},\\ y^2-y^{k+1},\\ \\dots,\\ y^k-y^{k+1} $$ are linearly independent.</p> <p>The convex hull of \\(k+1\\) affinely independent points is a \\(k\\)-dimensional simplex: - \\(k=1\\): line segment - \\(k=2\\): triangle - \\(k=3\\): tetrahedron/pyramid - etc.</p>"},{"location":"chapter3/notes/#365-bfs-and-the-basic-simplex","title":"3.6.5 BFS and the \u201cbasic simplex\u201d","text":"<p>In problem (3.6), there are \\(m+1\\) equality constraints (\\(Ax=b\\) gives \\(m\\), plus \\(e^\\top x=1\\) gives one more). So a BFS corresponds to choosing \\(m+1\\) basic variables (equivalently \\(m+1\\) basic points).</p> <p>Geometrically: - A basis corresponds to selecting \\(m+1\\) lifted points \\((A_{B(i)},c_{B(i)})\\) that are affinely independent. - Their convex hull is an \\(m\\)-dimensional simplex, called the basic simplex. - The current point \\((b,z)\\) is written as a convex combination of these basic points; the coefficients are the basic variables.</p> <p>The remaining points are nonbasic points.</p>"},{"location":"chapter3/notes/#366-change-of-basis-move-to-an-adjacent-simplex","title":"3.6.6 Change of basis = move to an adjacent simplex","text":"<p>A simplex pivot replaces one basic point by one nonbasic point: - one point enters the basic simplex (entering variable), - one point leaves it (leaving variable).</p> <p>So simplex \u201cwalks\u201d from one basic simplex to a neighboring one along the boundary of the convex hull.</p>"},{"location":"chapter3/notes/#367-dual-plane-and-reduced-costs-key-link","title":"3.6.7 Dual plane and reduced costs (key link)","text":"<p>Take the current basic simplex. The lifted basic points lie on an \\(m\\)-dimensional hyperplane in \\(\\mathbb{R}^{m+1}\\). The book calls the hyperplane through these basic points the dual plane.</p> <p>Now pick a candidate entering point \\((A_j,c_j)\\). - If \\((A_j,c_j)\\) lies below the dual plane, then moving to a new basis that includes it can lower the achieved height (cost). - If it lies above the dual plane, it is not profitable.</p> <p>The book states the geometric equivalence:</p> <p>\u201cPoint \\((A_j,c_j)\\) is below the dual plane\u201d \\(\\Longleftrightarrow\\) \u201cReduced cost of \\(x_j\\) is negative\u201d (for minimization).</p> <p>So reduced cost is literally a signed vertical distance from the dual plane to the candidate point.</p>"},{"location":"chapter3/notes/#368-pivoting-as-hinging-physical-analogy","title":"3.6.8 Pivoting as \u201chinging\u201d (physical analogy)","text":"<p>Think of the current basic simplex as a rigid face (triangle/pyramid/etc.). - The requirement line intersects it at the current BFS point. - When a new point enters, the simplex changes by \u201chinging\u201d about a shared edge (face),   moving the intersection point up or down.</p> <p>A profitable pivot is one that makes the new simplex lower (decreases cost).</p>"},{"location":"chapter3/notes/#369-example-310-why-simplex-can-be-fast","title":"3.6.9 Example 3.10 (why simplex can be fast)","text":"<p>For the case \\(m=1\\), the geometry becomes two-dimensional. The book gives an example where the optimal basis is found in two pivots, even though there may be many variables \\(n\\).</p> <p>This motivates the practical observation: - simplex often needs few pivots relative to problem size, despite worst-case examples.</p>"},{"location":"chapter3/notes/#37-computational-efficiency-of-the-simplex-method","title":"3.7 Computational efficiency of the simplex method","text":"<p>The computational effort of simplex depends on:</p> <ol> <li>Work per iteration</li> <li>Number of iterations</li> </ol> <p>Section 3.3 already discussed (1). Here the focus is mostly on (2), while summarizing (1).</p>"},{"location":"chapter3/notes/#371-work-per-iteration-recap","title":"3.7.1 Work per iteration (recap)","text":"<ul> <li>Full tableau: about \\(O(mn)\\) arithmetic per iteration (updating a size \\(m\\times n\\) tableau).</li> <li>Revised simplex: basis solves and updates are about \\(O(m^2)\\) per iteration, with reduced-cost computation ranging from \\(O(m)\\) to \\(O(mn)\\) depending on pricing strategy and sparsity.</li> </ul> <p>So per-iteration cost can be controlled; the main theoretical difficulty is the number of pivots.</p>"},{"location":"chapter3/notes/#372-worst-case-number-of-iterations-can-be-exponential","title":"3.7.2 Worst-case number of iterations can be exponential","text":"<p>The book discusses a construction where simplex visits an exponentially long path of adjacent vertices.</p> <p>Start with the \\(n\\)-dimensional unit cube: $$ 0\\le x_i\\le 1,\\quad i=1,\\dots,n, $$ which has \\(2^n\\) vertices.</p> <p>There exists a spanning path that visits all \\(2^n\\) vertices by moving along edges (Figures 3.8(a),(b) illustrate this for \\(n=2,3\\)). With an appropriate objective and a suitable pivot rule, simplex can be forced to follow such a path.</p> <p>To make the objective strictly decrease each step, the book perturbs the cube with constraints (for some small \\(\\varepsilon\\in(0,1/2)\\)):</p> \\[ \\varepsilon\\le x_1\\le 1, \\tag{3.7} \\] \\[ \\varepsilon x_{i-1} \\le x_i \\le 1-\\varepsilon x_{i-1}, \\quad i=2,\\dots,n. \\tag{3.8} \\] <p>Then one can choose a cost (book uses \\(-x_n\\)) so that the simplex method decreases cost at every move along that spanning path.</p> <p>Theorem 3.3 (worst case). For the LP that minimizes \\(-x_n\\) subject to (3.7)\u2013(3.8): - the feasible set has \\(2^n\\) vertices, - there is a pivot rule under which simplex requires \\(2^n-1\\) basis changes before terminating.</p> <p>So worst-case pivots are exponential in \\(n\\).</p>"},{"location":"chapter3/notes/#373-diameter-of-polyhedra-and-the-hirsch-conjecture","title":"3.7.3 Diameter of polyhedra and the Hirsch conjecture","text":"<p>To relate \u201cnumber of pivots\u201d to geometry, define adjacency graph distance.</p> <ul> <li>Two vertices are adjacent if connected by an edge.</li> <li>Let \\(d(x,y)\\) be the minimum number of edges in a path from \\(x\\) to \\(y\\).</li> <li>The diameter of a polyhedron \\(P\\) is   $$   D(P) = \\max_{x,y\\text{ vertices}} d(x,y).   $$</li> </ul> <p>Define: - \\(\\Delta(n,m)\\) = maximum possible diameter over all bounded polyhedra in \\(\\mathbb{R}^n\\) described by \\(m\\) linear inequalities. - \\(\\Delta_u(n,m)\\) = same but allowing unbounded polyhedra.</p> <p>If every bounded polyhedron had diameter polynomial in \\(m,n\\), then there would always exist a short vertex-to-vertex path, suggesting that an edge-following method might be polynomial with a good rule. However, simplex\u2019s pivot rule might still fail to find the short path.</p> <p>Hirsch conjecture (as stated in the book): $$ \\Delta(n,m) \\le m-n. $$</p> <p>The book notes: - Hirsch is false for unbounded polyhedra (Klee and Walkup, 1967), and gives a lower bound:   $$   \\Delta_u(n,m) \\ge m-n+\\left\\lfloor\\frac{n}{5}\\right\\rfloor.   $$ - Worst-case upper bounds exist but are still large; the book quotes bounds of the form   $$   \\Delta(n,m) \\le \\Delta_u(n,m) \\le m^{1+\\log_2 n} = (2n)^{\\log_2 m}.   $$</p> <p>So the true growth of diameters (and its relation to polynomial-time behavior) is subtle.</p>"},{"location":"chapter3/notes/#374-average-case-behavior-why-practice-looks-good","title":"3.7.4 Average-case behavior (why practice looks good)","text":"<p>Worst-case results do not explain practical performance.</p> <p>The \u201caverage-case\u201d question is hard because: - we must define a probability distribution over LP instances, - different distributions give different behavior.</p> <p>The book mentions positive results under certain random models: - randomly chosen \\(c\\), \\(A\\), and \\(b\\), - constraints of the form \\(a_i^\\top x\\le b_i\\) or \\(a_i^\\top x\\ge b_i\\) with equal probability, - conditioned on feasibility, simplex can have expected pivot count that grows moderately (often near linear in \\(n\\) for fixed \\(m\\)).</p> <p>Main point: - simplex is not polynomial in the worst case, but - it is often efficient in practice, and explaining this rigorously is a major research topic.</p>"},{"location":"chapter3/notes/#38-summary-what-to-memorize-for-exams","title":"3.8 Summary (what to memorize for exams)","text":"<p>This chapter developed the simplex method as a complete algorithm for LPs in standard form.</p>"},{"location":"chapter3/notes/#381-the-big-picture-of-simplex","title":"3.8.1 The \u201cbig picture\u201d of simplex","text":"<ul> <li>The feasible set \\(P=\\{x\\mid Ax=b,\\ x\\ge 0\\}\\) is a polyhedron.</li> <li>Simplex moves from one basic feasible solution (vertex/extreme point) to another along edges.</li> <li>Each pivot is a basis change: one nonbasic variable enters, one basic variable leaves.</li> </ul>"},{"location":"chapter3/notes/#382-core-mathematical-ingredients","title":"3.8.2 Core mathematical ingredients","text":""},{"location":"chapter3/notes/#a-reduced-costs-and-optimality-section-31","title":"(a) Reduced costs and optimality (Section 3.1)","text":"<p>For a basis \\(B\\):</p> <ul> <li>Basic solution: \\(x_B=B^{-1}b\\), \\(x_N=0\\).</li> <li>Reduced cost:   $$   \\bar c_j = c_j - c_B^\\top B^{-1}A_j.   $$</li> </ul> <p>Key optimality condition: - If \\(\\bar c_j\\ge 0\\) for all nonbasic \\(j\\), the BFS is optimal. - If BFS is nondegenerate and some \\(\\bar c_j&lt;0\\), then there is a feasible improving edge.</p>"},{"location":"chapter3/notes/#b-pivot-mechanics-section-32","title":"(b) Pivot mechanics (Section 3.2)","text":"<p>Given an entering index \\(j\\):</p> <ul> <li>Direction: \\(d_B=-B^{-1}A_j\\) and \\(d_j=1\\).</li> <li> <p>Ratio test:   $$   \\theta^* = \\min_{i: (B^{-1}A_j)_i&gt;0}\\ \\frac{(x_B)_i}{(B^{-1}A_j)_i}.   $$</p> </li> <li> <p>Leaving variable is the one achieving the minimum.</p> </li> <li>If no component of \\(B^{-1}A_j\\) is positive, then \\(d\\ge 0\\) and \\(c^\\top d&lt;0\\) \u21d2 objective is unbounded below (\\(-\\infty\\)).</li> </ul>"},{"location":"chapter3/notes/#383-implementation-styles-section-33","title":"3.8.3 Implementation styles (Section 3.3)","text":"<p>Three equivalent bookkeeping styles:</p> <ol> <li>Naive: form \\(B^{-1}\\) explicitly each iteration (mostly educational).</li> <li>Revised simplex: store \\(B\\) (or its factorization) and compute:</li> <li>\\(x_B\\) by solving \\(Bx_B=b\\),</li> <li>simplex multipliers \\(p\\) by solving \\(B^\\top p=c_B\\),</li> <li>reduced costs by \\(\\bar c_j=c_j-p^\\top A_j\\),</li> <li>pivot column by solving \\(Bu=A_j\\).</li> <li>Full tableau: maintain a tableau containing \\(B^{-1}A\\) and \\(B^{-1}b\\), update by row operations.</li> </ol> <p>Memory/time tradeoff: - Full tableau uses \\(O(mn)\\) storage and about \\(O(mn)\\) work per pivot. - Revised simplex uses \\(O(m^2)\\) basis storage (plus sparse \\(A\\)) and can be much more efficient in large problems.</p>"},{"location":"chapter3/notes/#384-degeneracy-cycling-and-anticycling-section-34","title":"3.8.4 Degeneracy, cycling, and anticycling (Section 3.4)","text":"<p>Degeneracy causes: - \\(\\theta^*=0\\) pivots (basis changes without moving in \\(x\\)), - potential cycling (infinite repetition of bases).</p> <p>Anticycling rules: - Lexicographic pivoting: break ratio ties by lexicographic row comparison; guarantees no repetition. - Bland\u2019s rule: smallest-index entering variable; smallest-index leaving among ratio ties; guarantees termination.</p>"},{"location":"chapter3/notes/#385-how-to-start-initial-bfs-section-35","title":"3.8.5 How to start: initial BFS (Section 3.5)","text":"<p>If a BFS is not obvious: - Solve a Phase I auxiliary LP using artificial variables \\(y\\ge 0\\):</p> \\[ \\min\\ \\sum_{i=1}^m y_i \\quad \\text{s.t.}\\quad Ax+y=b,\\; x\\ge 0,\\ y\\ge 0. \\] <ul> <li>If the optimal Phase I value is \\(&gt;0\\): original LP infeasible.</li> <li>If Phase I optimum is \\(0\\): obtain a feasible basis for Phase II (drive artificial variables out if needed).</li> </ul> <p>Alternative: big-\\(M\\) method (one-phase), but can be numerically delicate.</p>"},{"location":"chapter3/notes/#386-geometry-intuition-section-36","title":"3.8.6 Geometry intuition (Section 3.6)","text":"<p>With the convexity constraint \\(e^\\top x=1\\): - feasible solutions correspond to intersections of a vertical requirement line through \\(b\\) with the convex hull of lifted points \\((A_i,c_i)\\), - each basis corresponds to an \\(m\\)-dimensional basic simplex (simplex face), - reduced costs determine whether a candidate point lies below the dual plane (profitable entering variable).</p>"},{"location":"chapter3/notes/#387-efficiency-story-section-37","title":"3.8.7 Efficiency story (Section 3.7)","text":"<p>Simplex performance depends on: - work per iteration (tableau vs revised simplex), - number of iterations.</p> <p>Theory: - worst-case number of pivots can be exponential (\\(2^n-1\\) for certain families), - geometric concepts like polyhedron diameter relate to shortest possible vertex paths, - average-case behavior is more subtle and depends on the distribution of instances, but simplex is often efficient in practice.</p>"}]}